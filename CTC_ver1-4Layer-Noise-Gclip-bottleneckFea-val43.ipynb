{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC version 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Paper Reference\n",
    "- [Connectionist Temporal Classification (IDSIA)](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf)\n",
    "\n",
    "\n",
    "### English Test\n",
    "First we test English\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n",
      "/home/pika/nntools/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "# Include And \n",
    "import sys\n",
    "sys.path.append(\"/home/pika/nntools/\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from theano_toolkit import utils as U\n",
    "from theano_toolkit import updates\n",
    "from theano.printing import Print\n",
    "from theano_toolkit.parameters import Parameters\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import ctc_cost_2\n",
    "\n",
    "import cPickle\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concate\n",
      "Dataset Finish : \n",
      "total x_train = 3851\n",
      "[[ 0.30875856  3.77019548  0.02795531 ...,  0.12686451  0.15877695\n",
      "  -0.94809389]\n",
      " [ 0.54844403  2.00368309  1.21554422 ...,  0.08752131 -0.99940634\n",
      "   0.11953995]\n",
      " [ 0.74679321  1.01089871  1.99684393 ..., -0.29566044 -2.02656507\n",
      "   0.68761522]\n",
      " ..., \n",
      " [ 1.76844943 -1.40381742 -0.66371351 ..., -0.63197482 -0.1331615\n",
      "   0.24962384]\n",
      " [ 1.93053758 -1.46547914 -0.39516619 ..., -0.40233928  0.06236838\n",
      "  -0.19390136]\n",
      " [ 2.00433135 -1.49505806 -0.54028153 ..., -0.18457986  0.12119711\n",
      "  -0.42793319]]\n",
      "total y_train = 3851\n",
      "[ 24.  19.  22.   5.   0.   9.  12.   5.  28.   0.  18.   2.   0.  24.  19.\n",
      "  22.   5.   0.   9.  12.   5.  28.   0.   9.   5.  28.   0.  18.  11.  11.\n",
      "   0.  19.  12.   0.  24.  19.   6.  17.   8.   0.  16.  12.  18.   0.   9.\n",
      "  12.   5.  28.]\n",
      "total x_valid = 67\n",
      "[[ 0.91447097  4.14750957 -0.04186375 ...,  0.25524774  0.18128966\n",
      "   0.78523731]\n",
      " [ 0.94787949  3.73508501 -0.03330386 ...,  0.12123027  0.25109428\n",
      "   0.96071202]\n",
      " [ 1.05348313  3.65084338 -0.09161393 ..., -0.15304102  0.56191581\n",
      "   0.36370787]\n",
      " ..., \n",
      " [ 1.53992641  0.40703833  0.29321557 ...,  0.32870886 -0.9911406\n",
      "   0.64526808]\n",
      " [ 1.65826416  0.29736036  0.24873212 ...,  0.31267083 -0.4116033\n",
      "   0.44444811]\n",
      " [ 1.81673515  0.07878674  0.17473722 ...,  0.10852207  0.25881964\n",
      "   0.24393523]]\n",
      "total y_valid = 67\n",
      "[ 24.  19.  22.   5.   0.  14.   5.  24.   7.   5.  19.   0.  19.  12.  19.\n",
      "  12.  28.  18.   0.  21.   5.  10.  14.   5.  18.  19.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Doing : \n",
    "    - Read data and return x_train, y_train\n",
    "'''\n",
    "# TRAINING_ACOUSTIC_FEATURE = \"./dataCebuano/train.f0_ffv_fbank.fea\"\n",
    "# TRAINING_ACOUSTIC_FEATURE = \"./dataCebuano/train_Normalized_SELFVALID.ark\"\n",
    "TRAINING_ACOUSTIC_FEATURE = \"./dataCebuano/tlkagk/train.dnn.fea.lessThen500.ark\"\n",
    "TRAINING_LABELS = \"./dataCebuano/train.txt\"\n",
    "CHAR_CORPUS = \"./dataCebuano/corpus.txt\"\n",
    "\n",
    "\n",
    "def readDataSet () :\n",
    "    \n",
    "    # -- Mapping char to integer and reverse\n",
    "    char_map = {}    # eg. { a -> 1 }\n",
    "    char_unMap = {}  # eg. { 1 -> a }\n",
    "    with open( CHAR_CORPUS, 'r' ) as f:\n",
    "        char_index = 0\n",
    "        for lines in f:\n",
    "            char_map  [ lines[:-1] ] = char_index\n",
    "            char_unMap[ char_index ] = lines[:-1]\n",
    "            char_index += 1\n",
    "        # add Blank, take '.' as blank (not space)\n",
    "        char_map['.'] = char_index\n",
    "        char_unMap[char_index] = '.'\n",
    "    \n",
    "    \n",
    "    # -- Reading X = id -> [ list of feature ] \n",
    "    idMapX = {}\n",
    "    # idDeck = []\n",
    "    id_valid = []\n",
    "    id_train = []\n",
    "    id = ''\n",
    "    \n",
    "    tmp_l = 0;\n",
    "    with open( TRAINING_ACOUSTIC_FEATURE, 'r') as f:\n",
    "        for lines in f: \n",
    "            if '[' in lines :\n",
    "                id = lines.split(' ')[0]\n",
    "                if id.split('_')[0] == \"10647\" or id.split('_')[0] == \"11581\" or id.split('_')[0] == \"11673\":\n",
    "                    id_valid.append(id)\n",
    "                else :\n",
    "                    id_train.append(id)\n",
    "                idMapX[id] = []\n",
    "            elif ']' in lines :\n",
    "                if( len( lines.split('\\n')[0].split(' ') [0:-1] ) != 90 ) :\n",
    "                    print \"err\"\n",
    "                    return \n",
    "                idMapX[id].append([float(x) for x in lines.split('\\n')[0].split(' ') [:-1] ])\n",
    "            else :\n",
    "                if( len( lines.split('\\n')[0].split(' ')) != 90 ):\n",
    "                    print len( lines.split('\\n')[0].split(' '))\n",
    "                    print lines.split('\\n')[0].split(' ')\n",
    "                    print lines\n",
    "                    print id\n",
    "                    print \"fuck\"\n",
    "                    return\n",
    "                idMapX[id].append([float(x) for x in lines.split('\\n')[0].split(' ') ])\n",
    "    \n",
    "    # Move to  3 - main - 3\n",
    "    '''\n",
    "    monoDimension = 33;\n",
    "    zeroFeature = [0] * monoDimension\n",
    "    #print len( zeroFeature ) \n",
    "    #print zeroFeature\n",
    "    for id in idDeck:\n",
    "        # print id\n",
    "        tmpIdMapXWithID = []\n",
    "        if len(idMapX[id]) < 6 :\n",
    "            print \"ERROR in length\"\n",
    "            return \n",
    "        for idxX in xrange( len(idMapX[id]) ):\n",
    "            if idxX == 0:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + zeroFeature + zeroFeature + zeroFeature + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )    \n",
    "            elif idxX == 1:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + zeroFeature + zeroFeature + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            elif idxX == 2:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + zeroFeature + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            elif idxX == 3:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            elif idxX == 4:\n",
    "                tmpIdMapXWithID.append( zeroFeature + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            \n",
    "            elif idxX == (len(idMapX[id]) - 5 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 4 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + zeroFeature + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 3 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + zeroFeature + zeroFeature + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 2 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + zeroFeature + zeroFeature + zeroFeature + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 1 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + zeroFeature + zeroFeature + zeroFeature + zeroFeature + zeroFeature )  \n",
    "            else:\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "        idMapX[id] = tmpIdMapXWithID\n",
    "    '''\n",
    "    \n",
    "    print \"Concate\"\n",
    "    \n",
    "    # Standardlize X (Done in PrepreProcessing )\n",
    "   \n",
    "    # Shuffle the speaker to slice training and validation\n",
    "    '''\n",
    "    totalSpeech = len(idMapX)\n",
    "    slicePointFive = totalSpeech // 20\n",
    "    shuffle(idDeck)\n",
    "    id_train = idDeck[slicePointFive+1:]\n",
    "    id_valid = idDeck[:slicePointFive]\n",
    "    '''\n",
    "    idMapY = {} \n",
    "    \n",
    "    # -- Reading Y = id -> [ Sentence(list of char[int] ) ]\n",
    "    with open( TRAINING_LABELS, 'r') as f:\n",
    "        specific = '?'\n",
    "        for lines in f:\n",
    "            yWithId = lines.split('\\n')[0].split(' ', 1)\n",
    "            idMapY[ yWithId[0] ] = []\n",
    "            judgeSpecific = False\n",
    "            for char in yWithId[1] :\n",
    "                if char == '<' :\n",
    "                    judgeSpecific = True;                \n",
    "                if( judgeSpecific ):\n",
    "                    if char == '>':\n",
    "                        idMapY[ yWithId[0] ].append( char_map[specific] )\n",
    "                        judgeSpecific = False\n",
    "                else :\n",
    "                    idMapY[ yWithId[0] ].append( char_map[char] )\n",
    "    \n",
    "    # -- concatenate X and Y\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    \n",
    "    for id in id_train:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_train.append( floatX( concateId )  )\n",
    "        y_train.append( floatX( idMapY[id]) )\n",
    "        \n",
    "    for id in id_valid:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_valid.append( floatX( concateId ) )\n",
    "        y_valid.append( floatX( idMapY[id]) )\n",
    "        \n",
    "    print \"Dataset Finish : \"\n",
    "    print \"total x_train = \" + str(len(x_train))\n",
    "    print x_train[0]\n",
    "    print \"total y_train = \" + str(len(y_train))\n",
    "    print y_train[0]\n",
    "    print \"total x_valid = \" + str(len(x_valid))\n",
    "    print x_valid[0]\n",
    "    print \"total y_valid = \" + str(len(y_valid))\n",
    "    print y_valid[0]\n",
    "    \n",
    "    return char_map, char_unMap, x_train, y_train, x_valid, y_valid\n",
    "\n",
    "def floatX(x):\n",
    "    return np.asarray(x, dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "char_map, char_unMap, x_train, y_train, x_valid, y_valid = readDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 90)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 500 -> 3722 / 195\n",
    "# 600 -> 3821 / 201\n",
    "# 3000 -> 3937 / 207\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max( [X.shape[0] for X in x_train] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py\n",
    "    \n",
    "    Convert a list of matrices into batches of uniform length\n",
    "    :parameters:\n",
    "        - X : list of np.ndarray\n",
    "            List of matrices\n",
    "        - length : int\n",
    "            Desired sequence length.  Smaller sequences will be padded with 0s,\n",
    "            longer will be truncated.\n",
    "        - batch_size : int\n",
    "            Mini-batch size\n",
    "    :returns:\n",
    "        - X_batch : np.ndarray\n",
    "            Tensor of time series matrix batches,\n",
    "            shape=(n_batches, length, batch_size, n_features)\n",
    "        - X_mask : np.ndarray\n",
    "            shape=(n_batches, length, batch_size)\n",
    "            Mask denoting whether to include each time step of each time series\n",
    "            matrix\n",
    "    '''\n",
    "    \n",
    "def make_batches_X(X, length, batch_size=30):\n",
    "    n_batches = len(X)//batch_size\n",
    "    X_batch = np.zeros( (n_batches, batch_size, length, X[0].shape[1]),\n",
    "                         dtype=theano.config.floatX)\n",
    "    X_mask  = np.zeros( (n_batches, length, batch_size ), \n",
    "                         dtype=theano.config.floatX)\n",
    "    \n",
    "    for b in range(n_batches): \n",
    "        for n in range(batch_size): # go thorough batch size       \n",
    "            X_m = X[b*batch_size + n] # seq_length X feature dim            \n",
    "            X_batch[b, n, :X_m.shape[0]] = X_m[:length]\n",
    "            X_mask[b, :X_m.shape[0], n] = 1\n",
    "            \n",
    "    return X_batch, X_mask\n",
    "\n",
    "def make_batches_Y( X, length, batch_size=30):\n",
    "    n_batches = len(X)//batch_size\n",
    "    \n",
    "    X_batch = np.zeros( (n_batches, length, batch_size ), dtype='float32')\n",
    "    \n",
    "    X_mask = np.zeros(X_batch.shape, dtype=theano.config.floatX)\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        for n in range(batch_size):\n",
    "            X_m = X[ b*batch_size + n ]\n",
    "            X_batch[b, :X_m.shape[0], n ] = X_m[:length]\n",
    "            X_mask[b, :X_m.shape[0], n] = 1\n",
    "    return X_batch, X_mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "\n",
    "# Find the longest sequence\n",
    "length_x = max( max( [X.shape[0] for X in x_train] ),\n",
    "                max( [X.shape[0] for X in x_valid] ))\n",
    "\n",
    "length_y = max( max( [X.shape[0] for X in y_train] ),\n",
    "                max( [X.shape[0] for X in y_valid] ))\n",
    "\n",
    "# Convert to batches of time series of uniform length\n",
    "# x_train_mask: seq_length X batch_size\n",
    "# y_train_mask: output_length X batch_size\n",
    "# y_pred_mask = x_train_mask, since pred by sequence\n",
    "x_train, x_train_mask = make_batches_X(x_train, length_x, batch_size)\n",
    "y_train, y_train_mask = make_batches_Y(y_train, length_y, batch_size)\n",
    "x_valid, x_valid_mask = make_batches_X(x_valid, length_x, batch_size)\n",
    "y_valid, y_valid_mask = make_batches_Y(y_valid, length_y, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 40, 501, 90)\n",
      "(96, 132, 40)\n",
      "(1, 40, 501, 90)\n",
      "(1, 132, 40)\n",
      "================mask================\n",
      "(96, 501, 40)\n",
      "(96, 132, 40)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_valid.shape\n",
    "print y_valid.shape\n",
    "print \"================mask================\"\n",
    "print x_train_mask.shape\n",
    "print y_train_mask.shape\n",
    "\n",
    "# print x_train_mask[0][400][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit by NLTK package of measure edit distance\n",
    "'''\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (c1 != c2)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def check_label_error( real , predict, transpositions=False):\n",
    "    ## length of real >= length of predict\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(predict)\n",
    "    len2 = len(real)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, predict, real, transpositions=transpositions)\n",
    "            \n",
    "    return lev[len1][len2]*1.0/len2\n",
    "\n",
    "\n",
    "def clean_up( y ):\n",
    "    \"\"\"\n",
    "    for final output clean up\n",
    "    B(a − ab−) = B(−aa − −abb) = aab\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            if y[i-1] != y[i]:\n",
    "                answer += y[i]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def remap_back( y , actual):\n",
    "    answer = \"\"\n",
    "    \n",
    "    for i in y:\n",
    "        answer += char_unMap[i]\n",
    "    \n",
    "    if not actual:\n",
    "        answer = clean_up(answer)\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def decode_all_actual( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = np.swapaxes( y, 0 , 1)[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "\n",
    "def decode_all_pred( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = y[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "def check_all( y, y_mask, y_pred, y_pred_mask, batch_size ):\n",
    "    \n",
    "    actual = decode_all_actual( y, y_mask, batch_size , True )\n",
    "    predict = decode_all_pred( y_pred, y_pred_mask, batch_size , False )\n",
    "    \n",
    "    error = 0.\n",
    "    for a,b in zip (actual, predict):\n",
    "        error += check_label_error(a,b)\n",
    "    \n",
    "    return error/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class end_to_end():  \n",
    "    def __init__(self, input_shape, max_seq_length, hidden_layer,\n",
    "                 batch, max_epochs, output_num_units,\n",
    "                 patience, up_learning_rate, file_name):\n",
    "        \n",
    "        self.input_shape = input_shape # [batch, dim]\n",
    "        \n",
    "        self.hidden_layer = hidden_layer # hidden [l1, l2, l3]\n",
    "        self.output_num_units = output_num_units # [ # of class ]\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.up_learning_rate = up_learning_rate\n",
    "                 \n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_params = None\n",
    "        \n",
    "        self.train_history_ = []\n",
    "        self.epochs = 0\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        \"\"\"\n",
    "        input data type\n",
    "        y_hat : T x B x C+1\n",
    "        y : L x B\n",
    "        y_hat_mask : T x B\n",
    "        y_mask : L x B\n",
    "        \"\"\"\n",
    "        \n",
    "        print \"Why model build so long ... \"\n",
    "        # T x B x F\n",
    "        # B X T X F (Lasagne format)\n",
    "        x = T.tensor3('X', dtype=theano.config.floatX)\n",
    "        # L x B\n",
    "        y = T.matrix ('y', dtype=theano.config.floatX)\n",
    "\n",
    "        # L x B\n",
    "        y_mask = T.matrix('y_mask', dtype=theano.config.floatX)\n",
    "        # T x B\n",
    "        y_hat_mask = T.matrix('y_hat_mask', dtype=theano.config.floatX)\n",
    "    \n",
    "        # Min/max sequence length\n",
    "        MAX_LENGTH = max_seq_length\n",
    "        \n",
    "        # Number of training sequences in each batch\n",
    "        N_BATCH = batch\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # dEEP lSTM\n",
    "        #===========================================================================================\n",
    "        print \"Input : (N_BATCH) : \" + str(N_BATCH) +\", ML :\" + str(MAX_LENGTH) + \"ss :\" + str(self.input_shape[1])\n",
    "        \n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        l_in     = lasagne.layers.InputLayer( shape=( N_BATCH, MAX_LENGTH, self.input_shape[1] ) )\n",
    "        l_in_gau = lasagne.layers.GaussianNoiseLayer( l_in, sigma=0.5 )\n",
    "        \n",
    "        # LSTM layer 1\n",
    "        l_forward_1   = lasagne.layers.LSTMLayer(l_in_gau, num_units=hidden_layer[0], learn_init=True, peepholes=True)\n",
    "        l_backward_1  = lasagne.layers.LSTMLayer(l_in_gau, num_units=hidden_layer[0], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_1 = ElemwiseSumLayer( [l_forward_1, l_backward_1] )\n",
    "\n",
    "        \n",
    "        # LSTM layer 2\n",
    "        l_forward_2   = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], learn_init=True, peepholes=True)\n",
    "        l_backward_2  = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_2 = ElemwiseSumLayer( [l_forward_2, l_backward_2] )\n",
    "        \n",
    "        '''\n",
    "        # LSTM layer 3\n",
    "        l_forward_3   = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], learn_init=True, peepholes=True)\n",
    "        l_backward_3  = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_3 = ElemwiseSumLayer( [l_forward_3, l_backward_3] )\n",
    "        \n",
    "        # LSTM layer 4\n",
    "        l_forward_4   = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], learn_init=True, peepholes=True)\n",
    "        l_backward_4  = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_4 = ElemwiseSumLayer( [l_forward_4, l_backward_4] )\n",
    "        '''\n",
    "        \n",
    "        # l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_4, (N_BATCH*MAX_LENGTH, hidden_layer[3])  )\n",
    "        l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_2, (N_BATCH*MAX_LENGTH, hidden_layer[1])  )\n",
    "\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # COMMON SETUP\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Our output layer is a simple dense connection\n",
    "        l_recurrent_out      = lasagne.layers.DenseLayer( l_reshape, num_units=output_num_units[0] , nonlinearity=lasagne.nonlinearities.identity)\n",
    "        \n",
    "        # Now, reshape the output back to the RNN format\n",
    "        l_out_shp            = lasagne.layers.ReshapeLayer( l_recurrent_out, (N_BATCH, MAX_LENGTH, output_num_units[0]) )\n",
    "        \n",
    "        # dimshuffle to shape format (input_seq_len, batch_size, num_classes + 1)\n",
    "        l_out_shp_ctc        = lasagne.layers.DimshuffleLayer( l_out_shp, (1, 0, 2))\n",
    "\n",
    "        l_out_softmax        = lasagne.layers.NonlinearityLayer( l_recurrent_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        l_out_softmax_shp    = lasagne.layers.ReshapeLayer( l_out_softmax, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # since we use gaussian noise in input, False means use noise, True means dont use noise\n",
    "        output_lin_ctc_train = lasagne.layers.get_output( l_out_shp_ctc, x, deterministic=False)\n",
    "        output_softmax_train = lasagne.layers.get_output( l_out_softmax_shp, x, deterministic=False)\n",
    "        \n",
    "        output_lin_ctc_val   = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=True)\n",
    "        output_softmax_val   = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=True)\n",
    "        \n",
    "        self.all_params      = lasagne.layers.get_all_params(l_out_shp)\n",
    "    \n",
    "        # the CTC cross entropy between y and linear output network\n",
    "        pseudo_cost = ctc_cost_2.pseudo_cost(\n",
    "            y, output_lin_ctc_train, y_mask, y_hat_mask,\n",
    "            skip_softmax=True)\n",
    "        \n",
    "        \n",
    "        pseudo_cost_grad = T.grad(pseudo_cost.mean(), self.all_params)\n",
    "        true_cost        = ctc_cost_2.cost(y, output_softmax_train.dimshuffle(1, 0, 2), y_mask, y_hat_mask)\n",
    "        cost             = T.mean(true_cost)\n",
    "        \n",
    "        pseudo_cost_scaled_grads = lasagne.updates.total_norm_constraint(pseudo_cost_grad, 5)\n",
    "        updates_rms          = lasagne.updates.rmsprop(pseudo_cost_scaled_grads, self.all_params, learning_rate = self.up_learning_rate)\n",
    "        updates = lasagne.updates.apply_momentum(updates_rms,  self.all_params, momentum=0.9)\n",
    "        \n",
    "        self.train = theano.function(\n",
    "            inputs = [x, y, y_hat_mask, y_mask],\n",
    "            outputs = [ pseudo_cost.mean(), cost, output_softmax_train ],\n",
    "#             outputs = [output_lin_ctc, output_softmax, cost],\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        self.predict = theano.function( \n",
    "            inputs=[x], \n",
    "            outputs = [ output_softmax_val] \n",
    "        )\n",
    "\n",
    "        \n",
    "    # x_mask and y_mask is the same\n",
    "    # x_test_mask and y_test_mask is the same\n",
    "    def fit(self, x_train, y_train, x_test,  y_test , x_mask, y_mask, x_test_mask, y_test_mask ):\n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        for i in range(self.max_epochs):\n",
    "            self.epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            cs = 0.\n",
    "            pseudo_cs = 0.\n",
    "            for index in range(len(x_train)):\n",
    "                pseudo, cost, output_softmax = self.train( x_train[index] , y_train[index],\n",
    "                                                          x_mask[index], y_mask[index])\n",
    "                cs += cost\n",
    "                pseudo_cs += pseudo\n",
    "                gg = index\n",
    "                if index % 10 == 0:\n",
    "                    print index, pseudo, cost\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            cs /= len(x_train)\n",
    "            pseudo_cs /= len(x_train)\n",
    "            \n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "            if self.epochs <= 1:\n",
    "                previous_cs = \"-\"\n",
    "            else:\n",
    "                previous_cs = self.train_history_[-1]['cost']\n",
    "            print \"\\n===============================\"\n",
    "            print 'epoch {0} : pseudo= {1}, cost= {2}, previous_cost= {3}, train_time = {4} s'.format(self.epochs,\n",
    "                                                                                                      pseudo_cs, \n",
    "                                                                                                      cs,\n",
    "                                                                                                      previous_cs, \n",
    "                                                                                                      time() - t0)\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            t0 = time()\n",
    "            \n",
    "#             print np.argmax(output_softmax[:],axis=2)\n",
    "#             print decode_all( np.argmax(output_softmax[:],axis=2) , x_train_mask[gg],  batch_size , True)\n",
    "            \n",
    "            # save model first\n",
    "            # dont know the LER performance yet\n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "    \n",
    "\n",
    "            label_error_rate = 0.\n",
    "            print len(x_test)\n",
    "            y_actual = []\n",
    "            y_predict = []\n",
    "            for index in range( len(x_test) ) :\n",
    "\n",
    "                prepre = self.predict( x_test[index])\n",
    "                \n",
    "                label_error_rate += check_all( y_test[index], y_test_mask[index],\n",
    "                                              np.argmax(prepre[0],axis=2) , x_test_mask[index],\n",
    "                                              batch_size)\n",
    "                \n",
    "                if (( index + self.epochs ) % 4 == 0 ):\n",
    "                    ## print actual\n",
    "                    y_actual = decode_all_actual( y_test[index], y_test_mask[index], batch_size , True)\n",
    "\n",
    "                    ## print pred\n",
    "                    y_predict = decode_all_pred( np.argmax(prepre[0],axis=2) , x_test_mask[index],  batch_size , False)\n",
    "                \n",
    "            label_error_rate /= len(x_test)\n",
    "            self.train_history_.append({\"epoch\":self.epochs, \"cost\": cs, \"LER\":label_error_rate})\n",
    "            \n",
    "            print '\\t\\t\\t val_= {0}, test_time  = {1} s'.format(label_error_rate, time() - t0)\n",
    "            print \"===============================\\n\"\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            for a , b in zip (y_actual,y_predict):\n",
    "                print \"Target== \",a \n",
    "                print \"\\tAns => \",b\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            \"\"\"\n",
    "            should use cost do early stopping\n",
    "            \"\"\"\n",
    "            current_cs = self.train_history_[-1]['cost']\n",
    "            current_epoch = self.train_history_[-1]['epoch']\n",
    "            if current_cs < self.best_valid:\n",
    "                self.best_valid = current_cs\n",
    "                self.best_valid_epoch = current_epoch\n",
    "                self.best_params = [w.get_value() for w in self.all_params]\n",
    "                cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "            elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "                print \"\"\n",
    "                print \"Early stopping.\"\n",
    "                print self.best_valid_epoch,self.best_valid\n",
    "                print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)              \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "            \"\"\"\n",
    "            can not use label error rate do early stopping\n",
    "            \"\"\"\n",
    "#             current_ler = self.train_history_[-1]['LER']\n",
    "#             current_epoch = self.train_history_[-1]['epoch']\n",
    "#             if current_ler < self.best_valid:\n",
    "# #                 print \"********************* Now best ************************\"\n",
    "#                 sys.stdout.flush()\n",
    "#                 self.best_valid = current_ler\n",
    "#                 self.best_valid_epoch = current_epoch\n",
    "#                 self.best_params = [w.get_value() for w in self.all_params]\n",
    "# #                 cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "#             elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "#                 print \"\"\n",
    "#                 print \"Early stopping.\"\n",
    "#                 print self.best_valid_epoch,self.best_valid\n",
    "#                 print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n",
    "#                 sys.stdout.flush()                \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "\n",
    "#     def prediction(self, x, x_mask) :\n",
    "        \n",
    "#         abc =  self.predict(x, x_mask)\n",
    "        \n",
    "#         return np.argmax(abc[0], axis = 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/nntools/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n",
      "/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1017: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why model build so long ... \n",
      "Input : (N_BATCH) : 40, ML :501ss :90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ONE lAYER\n",
    "'''\n",
    "\n",
    "model2L_BRAND_Noise_momemtumClip_bottleneckfea = end_to_end (\n",
    "    input_shape      = (1, x_train.shape[3] ) , # batch of 1, (110, 30, 777, 117)\n",
    "    max_seq_length = x_train.shape[2],\n",
    "    hidden_layer     = [ 650, 650, 650, 650 ], # maximum layer to LSTM 3 layer only\n",
    "    batch            = batch_size, \n",
    "    max_epochs       = 300, \n",
    "    output_num_units = [ len(char_unMap) ],\n",
    "    up_learning_rate = 0.0001, \n",
    "    patience         = 7,\n",
    "    file_name = \"model2L_BRAND_Noise_momemtumClip_bottleneckfea\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "start training!!!!\n",
      " \n",
      "0 -41.3696784973 701.955688477\n",
      "10 184.33454895 239.494064331\n",
      "20 1061.46081543 inf\n",
      "30 1076.11035156 inf\n",
      "40 838.116638184 940.44519043\n",
      "50 1061.4074707 1301.92089844\n",
      "60 1320.21228027 inf\n",
      "70 895.698852539 926.50958252\n",
      "80 1009.78448486 1086.21447754\n",
      "90 1082.69555664 inf\n",
      "\n",
      "===============================\n",
      "epoch 1 : pseudo= 870.027893116, cost= inf, previous_cost= -, train_time = 1412.65254021 s\n",
      "1\n",
      "\t\t\t val_= 0.96177217357, test_time  = 0.567327976227 s\n",
      "===============================\n",
      "\n",
      "0 1008.90802002 1097.22888184\n",
      "10 1148.34484863 inf\n",
      "20 1188.77416992 inf\n",
      "30 830.180358887 inf\n",
      "40 989.983215332 1080.94189453\n",
      "50 1165.68823242 inf\n",
      "60 878.101196289 900.893859863\n",
      "70 770.607116699 976.55090332\n",
      "80 1060.52685547 1059.70800781\n",
      "90 1047.16003418 inf\n",
      "\n",
      "===============================\n",
      "epoch 2 : pseudo= 1075.81285985, cost= inf, previous_cost= inf, train_time = 1405.69238281 s\n",
      "1\n",
      "\t\t\t val_= 0.947931417299, test_time  = 0.561805963516 s\n",
      "===============================\n",
      "\n",
      "0 1322.58581543 inf\n",
      "10 1453.35913086 1897.55737305\n",
      "20 889.750915527 990.330444336\n",
      "30 890.891418457 842.42175293\n",
      "40 554.725280762 613.603759766\n",
      "50 673.902038574 727.969482422\n",
      "60 524.07434082 512.986938477\n",
      "70 478.044586182 496.287750244\n",
      "80 530.287780762 541.503723145\n",
      "90 617.548400879 645.840270996\n",
      "\n",
      "===============================\n",
      "epoch 3 : pseudo= 743.464541117, cost= inf, previous_cost= inf, train_time = 1396.68129086 s\n",
      "1\n",
      "\t\t\t val_= 0.944472547672, test_time  = 0.55895614624 s\n",
      "===============================\n",
      "\n",
      "0 826.142211914 863.332824707\n",
      "10 514.951599121 575.74206543\n",
      "20 654.264831543 693.655883789\n",
      "30 344.352783203 441.627502441\n",
      "40 246.945022583 347.938934326\n",
      "50 318.870666504 372.858459473\n",
      "60 218.600616455 291.701599121\n",
      "70 126.864257812 249.03427124\n",
      "80 106.158790588 191.256881714\n",
      "90 380.933197021 371.428039551\n",
      "\n",
      "===============================\n",
      "epoch 4 : pseudo= 321.225656112, cost= inf, previous_cost= inf, train_time = 1387.07870412 s\n",
      "1\n",
      "\t\t\t val_= 0.938558375507, test_time  = 0.562386989594 s\n",
      "===============================\n",
      "\n",
      "Target==  unsa kauban niniyo pagkaon\n",
      "\tAns =>  awa\n",
      "Target==  unsa may gisud-an sa mama nimo\n",
      "\tAns =>   awa\n",
      "Target==  sila ati nimo naa ba dinha' si ati nimo\n",
      "\tAns =>   a\n",
      "Target==  si ati ati nimo si ati nimo nene naa dinha'\n",
      "\tAns =>  owawa\n",
      "Target==  ang bana ni ati nimo nene asa man\n",
      "\tAns =>  o o\n",
      "Target==  unsa gitrabaho didto nag-type siya\n",
      "\tAns =>   awa\n",
      "Target==  unsa man ila nga trabaho dinha' naa ing overtime\n",
      "\tAns =>  awa\n",
      "Target==  ikaw unsa may ikaisturiya nimo sa imo nga kaugalingon\n",
      "\tAns =>  oan\n",
      "Target==  okay ba kaha ka sa imo nga kaugalingon\n",
      "\tAns =>  an\n",
      "Target==  sigi lang maningkamot lang ta og ?\n",
      "\tAns =>  a\n",
      "Target==  paningkamot lang ta mag-apply og trabaho para aron makatrabaho ta\n",
      "\tAns =>  an\n",
      "Target==  uniya' ? nagaulan pa ba kaha dinha' wala' pa\n",
      "\tAns =>   wa\n",
      "Target==  wala' ing ulan piro diri mura og kaulanon na hapit\n",
      "\tAns =>   awa\n",
      "Target==  ? ? unsa man nawala' man nadawat\n",
      "\tAns =>   ow\n",
      "Target==  ? tapok ?\n",
      "\tAns =>  an\n",
      "Target==  kay si mama human na man kadto og\n",
      "\tAns =>   awa\n",
      "Target==  kaon kay milakaw na man kadto\n",
      "\tAns =>  awa\n",
      "Target==  trabaho man wala'\n",
      "\tAns =>  haha\n",
      "Target==  nagtrabaho og kuan semitrix\n",
      "\tAns =>   wawa\n",
      "Target==  ? drive\n",
      "\tAns =>  o\n",
      "Target==  pirmaninti man na ang overtime niya\n",
      "\tAns =>  a\n",
      "Target==  ? ?\n",
      "\tAns =>   wa\n",
      "Target==  ? hulaton lang nako' og tawag miani ko' nga\n",
      "\tAns =>   awa\n",
      "Target==  miani pag-apply miana' man kadto ang manager nawala'\n",
      "\tAns =>   wan\n",
      "Target==  duha dili' oy isa' ra oy lain pod na ang duha oy gabaan ta og duha\n",
      "\tAns =>  ow\n",
      "Target==  og imuha dili' man ka kaduol\n",
      "\tAns =>   awa\n",
      "Target==  ? puwidi puwidi puwidi piro dipindi\n",
      "\tAns =>  oaoa\n",
      "Target==  aw ikaw pangita' lagi sa imuha mura man ka og dili' kabungol\n",
      "\tAns =>  ow\n",
      "Target==  aw dili' na ga magkuan na lang ko' ga fire dancer biya' ko' ga\n",
      "\tAns =>   a\n",
      "Target==  ? ako' ang si star screw\n",
      "\tAns =>  awa\n",
      "Target==  naa pod si bumble bee\n",
      "\tAns =>   awa\n",
      "Target==  ang pinaka- ang pinakamaayo nga kuan dinha' si kuan benhur luy\n",
      "\tAns =>   oa\n",
      "Target==  ? si benhur luy kay\n",
      "\tAns =>   awa\n",
      "Target==  ko' nga ga- ko' nga gabii gani' nakit-an man si benhur luy kadto nga nagkaon og kuan chinese foods\n",
      "\tAns =>  bn\n",
      "Target==  ingon ka-duha ko' nga duha sila i-share gayod na siya\n",
      "\tAns =>   wa\n",
      "Target==  dili' ba kana' nga patusik lang ba dili' man nako'\n",
      "\tAns =>   awawawa\n",
      "Target==  ingon miana' makatapad ta ba i-share lang na siya\n",
      "\tAns =>   awa\n",
      "Target==  piro sa akua dili' ka magpa-share na\n",
      "\tAns =>  awa\n",
      "Target==  uniya' mu-practice gihapon ka uniya' ga\n",
      "\tAns =>  h hawa\n",
      "Target==  ? ? kana' diay iniyo nga kuan miana' diay unsa man diay iniyo nga team miana' diay ?\n",
      "\tAns =>  o\n",
      "0 189.449264526 279.851165771\n",
      "10 205.116851807 247.017730713\n",
      "20 249.547561646 298.573425293\n",
      "30 122.450881958 189.31199646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fd9e6f0715fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time model2L_BRAND_Noise_momemtumClip_bottleneckfea.fit( x_train, y_train, x_valid, y_valid, x_train_mask, y_train_mask , x_valid_mask, y_valid_mask )'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2302\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2303\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2304\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2223\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2224\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2225\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2226\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1163\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-b802050c5e6f>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, x_test, y_test, x_mask, y_mask, x_test_mask, y_test_mask)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                 pseudo, cost, output_softmax = self.train( x_train[index] , y_train[index],\n\u001b[1;32m--> 159\u001b[1;33m                                                           x_mask[index], y_mask[index])\n\u001b[0m\u001b[0;32m    160\u001b[0m                 \u001b[0mcs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[0mpseudo_cs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpseudo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    670\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    671\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 672\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    659\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/pika/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.9-64/scan_perform/mod.cpp:3537)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/pika/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoContext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time model2L_BRAND_Noise_momemtumClip_bottleneckfea.fit( x_train, y_train, x_valid, y_valid, x_train_mask, y_train_mask , x_valid_mask, y_valid_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "＃ Not used below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Process once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpus without develope data:\n",
      "set([' ', \"'\", '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
      "Total len is : 31\n",
      "Done Write to [./dataCebuano/corpus.txt]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "[ Doing : ]\n",
    "    - Read character from *.txt to build the corpus \n",
    "\n",
    "[ Result : ]\n",
    "\n",
    "-- Without Develope Data:\n",
    "\n",
    "Building corpus without develope data:\n",
    "set([' ', \"'\", '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "Total len is : 31\n",
    "Done Write to [./dataCebuano/corpus.txt]\n",
    "\n",
    "-- With Develope Data:\n",
    "\n",
    "Building corpus with develope data:\n",
    "set([' ', \"'\", ')', '(', '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "Total len is : 33\n",
    "Done Write to [./dataCebuano/corpus.txt]\n",
    "'''\n",
    "\n",
    "TRAINING_LABEL = \"./dataCebuano/train.txt\"\n",
    "DEVELOPE_LABEL = \"./dataCebuano/dev.txt\"\n",
    "WRITING_TO = \"./dataCebuano/corpus.txt\"\n",
    "\n",
    "# Special part to build corpus\n",
    "def buildCharacterCorpus(withDevelopeData = False) :\n",
    "    corpus = set( )\n",
    "    with open(TRAINING_LABEL, 'r') as ft:\n",
    "        for lines in ft:\n",
    "            for word in lines.split('\\n')[0].split(' ')[1:]:\n",
    "                if word[0] == '<':\n",
    "                    # We take every <...> as <unk> \n",
    "                    unKnownTag = '?'\n",
    "                    corpus.add( unKnownTag ) \n",
    "                else:\n",
    "                    for char in word:\n",
    "                        corpus.add(char.lower())\n",
    "    \n",
    "    if( withDevelopeData ) :\n",
    "        with open(DEVELOPE_LABEL, 'r') as fd:\n",
    "            for lines in fd:\n",
    "                for word in lines.split('\\n')[0].split(' ')[1:]:\n",
    "                    if word[0] == '<':\n",
    "                        unKnownTag = '?'\n",
    "                        corpus.add( unKnownTag ) \n",
    "                        # corpus.add(word.split('\\n')[0].lower())\n",
    "                    else:\n",
    "                        for char in word:\n",
    "                            corpus.add(char.lower())\n",
    "    \n",
    "    corpus.add(' ')\n",
    "    print \"Building corpus with\" + (\"\" if withDevelopeData else \"out\") + \" develope data:\"\n",
    "    print corpus\n",
    "    print \"Total len is : \" + str(len(corpus))\n",
    "    \n",
    "    \n",
    "    with open(WRITING_TO, 'w+') as fw:\n",
    "        for item in corpus:\n",
    "            fw.write(item + '\\n')        \n",
    "    print \"Done Write to [\" + WRITING_TO + \"]\"\n",
    "    return corpus\n",
    "\n",
    "trainingCorpus = buildCharacterCorpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Max len is  501\n",
      "Each Length is  90\n",
      "Original Total training data 3918\n",
      "MIssion ComplEte\n"
     ]
    }
   ],
   "source": [
    "# import theano\n",
    "# from theano import tensor as T\n",
    "# from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search as gs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import repeat\n",
    "\n",
    "#proprocessor\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n",
    "\n",
    "TRAINING_DATA   = './dataCebuano/tlkagk/train.dnn.fea'\n",
    "# VALIDATION_DATA = './dataCebuano/dev.f0_ffv_fbank.fea'\n",
    "TRAIN_ARK  = './dataCebuano/tlkagk/train.dnn.fea.lessThen500.ark'\n",
    "# TRAIN_ARK  = './dataCebuano/train_Normalized_SELFVALID_LenLessThen600.ark'\n",
    "\n",
    "\n",
    "def normalizeAndFilterData ( maxLength = 3000) :\n",
    "    # Dataset\n",
    "    x_data = []\n",
    "    x_name = []\n",
    "    record_seq = []\n",
    "\n",
    "    with open( TRAINING_DATA, 'r') as f:\n",
    "        for lines in f: \n",
    "            if '[' in lines :\n",
    "                id = lines.split(' ')[0]\n",
    "                x_name.append(id)\n",
    "                seq = 0\n",
    "            elif ']' in lines :\n",
    "                if seq > maxLength :\n",
    "                    x_name.pop()\n",
    "                    del x_data[-seq:]\n",
    "                else :\n",
    "                    seq += 1\n",
    "                    record_seq.append(seq)\n",
    "                    x_data.append([float(x) for x in lines.split(' ') [2:-1] ])\n",
    "            else :\n",
    "                seq += 1\n",
    "                x_data.append([float(x) for x in lines.split(' ') [2:-1] ])\n",
    "\n",
    "    print \"Normalized\"\n",
    "    x_data = preprocessing.scale( np.array(x_data) )\n",
    "\n",
    "    # x_data[separate+1]\n",
    "    # x_name\n",
    "    bitch = record_seq.index(max(record_seq))\n",
    "    print \"Max len is \", record_seq[bitch]\n",
    "    \n",
    "    print \"Each Length is \", len(x_data[bitch])\n",
    "    print \"Original Total training data\", len(x_name)\n",
    "\n",
    "    # trainF = open(TRAIN_ARK, 'w+')\n",
    "    with open(TRAIN_ARK, 'w+') as trainF:\n",
    "        currentSeq = 0\n",
    "        for nameIndex in range( len(x_name) ):\n",
    "\n",
    "            trainF.write(str(x_name[nameIndex])+\"  [\\n\")\n",
    "\n",
    "            for valueIndex in range( (record_seq[nameIndex]) ):\n",
    "                for featureIndex in range( len(x_data[0]) ):\n",
    "                    if ( featureIndex+1) != len(x_data[0] ) :\n",
    "                        trainF.write( str(x_data[ currentSeq  + valueIndex][featureIndex]) + \" \")\n",
    "                    else:\n",
    "                        trainF.write( str(x_data[ currentSeq  + valueIndex][featureIndex]) )\n",
    "                if valueIndex+1 == (record_seq[nameIndex]):\n",
    "                    trainF.write(\" ]\\n\")\n",
    "                else:\n",
    "                    trainF.write(\"\\n\")\n",
    "            currentSeq += record_seq[nameIndex]\n",
    "\n",
    "    print \"MIssion ComplEte\"\n",
    "\n",
    "\n",
    "normalizeAndFilterData(500)\n",
    "# ORIGINAL 3918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01      ,  0.002     ],\n",
       "       [ 1.20000005,  1.10000002]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
