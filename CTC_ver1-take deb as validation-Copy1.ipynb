{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC version 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Paper Reference\n",
    "- [Connectionist Temporal Classification (IDSIA)](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf)\n",
    "\n",
    "\n",
    "### English Test\n",
    "First we test English\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n",
      "/home/pika/nntools/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "# Include And \n",
    "import sys\n",
    "sys.path.append(\"/home/pika/nntools/\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from theano_toolkit import utils as U\n",
    "from theano_toolkit import updates\n",
    "from theano.printing import Print\n",
    "from theano_toolkit.parameters import Parameters\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import ctc_cost_2\n",
    "\n",
    "import cPickle\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Concate\n",
      "Dataset Finish : \n",
      "total x_train = 3918\n",
      "[[ 0.          0.          0.         ...,  0.23138127  0.61976272\n",
      "   1.60866237]\n",
      " [ 0.          0.          0.         ...,  0.31543753  0.83928865\n",
      "   2.12280607]\n",
      " [ 0.          0.          0.         ...,  0.5311352   1.02809012\n",
      "   1.97899318]\n",
      " ..., \n",
      " [-0.23737173 -0.50330025 -1.17488396 ...,  0.          0.          0.        ]\n",
      " [-0.27265871 -0.46600449 -1.17488396 ...,  0.          0.          0.        ]\n",
      " [-0.44909349 -0.65248322 -1.30096579 ...,  0.          0.          0.        ]]\n",
      "total y_train = 3918\n",
      "[ 24.  19.  22.   5.   0.  14.   5.  24.   7.   5.  19.   0.  19.  12.  19.\n",
      "  12.  28.  18.   0.  21.   5.  10.  14.   5.  18.  19.]\n",
      "total x_valid = 4157\n",
      "[[-1.48412788 -1.32281327 -1.62171197 ...,  0.41936278  0.861175\n",
      "   2.51489568]\n",
      " [-0.99721181 -1.39681101 -1.95588279 ...,  0.34756452  0.92658424\n",
      "   2.46758008]\n",
      " [-1.3102293  -1.76679909 -1.80736244 ...,  0.27932435  0.69767028\n",
      "   2.47513175]\n",
      " ..., \n",
      " [ 1.13258958  0.5556218   0.08627325 ...,  1.43581188  1.62581968\n",
      "   0.10516509]\n",
      " [ 0.98845166  0.43940517 -0.13650745 ...,  0.99968147  1.29053867\n",
      "   0.42814341]\n",
      " [ 0.88903505  0.2681343  -0.54493833 ...,  1.25104654  1.52039433\n",
      "   1.29349518]]\n",
      "total y_valid = 4157\n",
      "[ 24.  19.  12.  28.   5.   1.   0.  14.  24.  16.  24.  22.  25.   5.   0.\n",
      "  16.   5.  19.   0.  12.  16.  18.   0.  19.  10.   5.   0.   8.  29.   5.\n",
      "  16.   0.  10.   5.  19.  12.  19.   5.   0.  15.  24.  19.   0.  21.   5.\n",
      "  22.   5.  23.   0.  14.   5.   9.  25.  18.   0.  15.  24.  19.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Doing : \n",
    "    - Read data and return x_train, y_train\n",
    "'''\n",
    "# TRAINING_ACOUSTIC_FEATURE = \"./dataCebuano/train.f0_ffv_fbank.fea\"\n",
    "# TRAINING_ACOUSTIC_FEATURE = \"./dataCebuano/train_Normalized_SELFVALID.ark\"\n",
    "TRAINING_ACOUSTIC_FEATURE = \"./dataCebuano/train_Normalized_SELFVALID_LenLessThen500.ark\"\n",
    "VALIDATION_ACOUSTIC_FEATURE = \"./dataCebuano/dev_Normalized_SELFVALID_LenLessThen500.ark\"\n",
    "\n",
    "TRAINING_LABELS = \"./dataCebuano/train.txt\"\n",
    "VALIDATION_LABELS = \"./dataCebuano/dev.txt\"\n",
    "\n",
    "CHAR_CORPUS = \"./dataCebuano/corpus.txt\"\n",
    "\n",
    "\n",
    "def readDataSet () :\n",
    "    \n",
    "    # -- Mapping char to integer and reverse\n",
    "    char_map = {}    # eg. { a -> 1 }\n",
    "    char_unMap = {}  # eg. { 1 -> a }\n",
    "    with open( CHAR_CORPUS, 'r' ) as f:\n",
    "        char_index = 0\n",
    "        for lines in f:\n",
    "            char_map  [ lines[:-1] ] = char_index\n",
    "            char_unMap[ char_index ] = lines[:-1]\n",
    "            char_index += 1\n",
    "        # add Blank, take '.' as blank (not space)\n",
    "        char_map['.'] = char_index\n",
    "        char_unMap[char_index] = '.'\n",
    "    \n",
    "    \n",
    "    # -- Reading X = id -> [ list of feature ] \n",
    "    idMapX = {}\n",
    "    idDeckTrain = []\n",
    "    idDeckValid = []\n",
    "    \n",
    "    id = ''\n",
    "    \n",
    "    with open( TRAINING_ACOUSTIC_FEATURE, 'r') as f:\n",
    "        for lines in f: \n",
    "            if '[' in lines :\n",
    "                id = lines.split(' ')[0]\n",
    "                idDeckTrain.append(id)\n",
    "                idMapX[id] = []\n",
    "            elif ']' in lines :\n",
    "                if( len( lines.split('\\n')[0].split(' ') [0:-1] ) != 33 ) :\n",
    "                    print \"err\"\n",
    "                    return \n",
    "                idMapX[id].append([float(x) for x in lines.split('\\n')[0].split(' ') [:-1] ])\n",
    "            else :\n",
    "                if( len( lines.split('\\n')[0].split(' ')) != 33 ):\n",
    "                    print len( lines.split('\\n')[0].split(' '))\n",
    "                    print lines.split('\\n')[0].split(' ')\n",
    "                    print lines\n",
    "                    print id\n",
    "                    print \"fuck\"\n",
    "                    return\n",
    "                idMapX[id].append([float(x) for x in lines.split('\\n')[0].split(' ') ])\n",
    "    \n",
    "    with open( VALIDATION_ACOUSTIC_FEATURE, 'r') as f:\n",
    "        for lines in f: \n",
    "            if '[' in lines :\n",
    "                id = lines.split(' ')[0]\n",
    "                idDeckValid.append(id)\n",
    "                idMapX[id] = []\n",
    "            elif ']' in lines :\n",
    "                if( len( lines.split('\\n')[0].split(' ') [0:-1] ) != 33 ) :\n",
    "                    print \"err\"\n",
    "                    return \n",
    "                idMapX[id].append([float(x) for x in lines.split('\\n')[0].split(' ') [:-1] ])\n",
    "            else :\n",
    "                if( len( lines.split('\\n')[0].split(' ')) != 33 ):\n",
    "                    print len( lines.split('\\n')[0].split(' '))\n",
    "                    print lines.split('\\n')[0].split(' ')\n",
    "                    print lines\n",
    "                    print id\n",
    "                    print \"fuck\"\n",
    "                    return\n",
    "                idMapX[id].append([float(x) for x in lines.split('\\n')[0].split(' ') ])\n",
    "    \n",
    "        \n",
    "    # Move to  3 - main - 3\n",
    "    \n",
    "    monoDimension = 33;\n",
    "    zeroFeature = [0] * monoDimension\n",
    "    print len( zeroFeature ) \n",
    "    print zeroFeature\n",
    "    for id in idDeckTrain:\n",
    "        # print id\n",
    "        tmpIdMapXWithID = []\n",
    "        if len(idMapX[id]) < 10 :\n",
    "            print \"ERROR in length\"\n",
    "            return \n",
    "        for idxX in xrange( len(idMapX[id]) ):\n",
    "            if idxX == 0:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + zeroFeature + zeroFeature + zeroFeature + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )    \n",
    "            elif idxX == 1:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + zeroFeature + zeroFeature + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            elif idxX == 2:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + zeroFeature + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            elif idxX == 3:\n",
    "                tmpIdMapXWithID.append( zeroFeature + zeroFeature + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            elif idxX == 4:\n",
    "                tmpIdMapXWithID.append( zeroFeature + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "            \n",
    "            elif idxX == (len(idMapX[id]) - 5 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 4 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + zeroFeature + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 3 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + zeroFeature + zeroFeature + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 2 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + zeroFeature + zeroFeature + zeroFeature + zeroFeature )  \n",
    "            elif idxX == (len(idMapX[id]) - 1 ):\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + zeroFeature + zeroFeature + zeroFeature + zeroFeature + zeroFeature )  \n",
    "            else:\n",
    "                tmpIdMapXWithID.append( idMapX[id][idxX-5] + idMapX[id][idxX-4] + idMapX[id][idxX-3] + idMapX[id][idxX-2] + idMapX[id][idxX-1] + idMapX[id][idxX] + idMapX[id][idxX+1] + idMapX[id][idxX+2] + idMapX[id][idxX+3] + idMapX[id][idxX+4] + idMapX[id][idxX+5] )  \n",
    "        idMapX[id] = tmpIdMapXWithID\n",
    "    \n",
    "    print \"Concate\"\n",
    "    \n",
    "    # Standardlize X (Done in PrepreProcessing )\n",
    "   \n",
    "    # Shuffle the speaker to slice training and validation\n",
    "        # For slice Validation by training data\n",
    "        # totalSpeech = len(idMapX)\n",
    "        # slicePointFive = totalSpeech // 20\n",
    "        # shuffle(idDeckTrain)\n",
    "        # id_train = idDeckTrain[slicePointFive+1:]\n",
    "        # id_valid = idDeckTrain[:slicePointFive]\n",
    "    # For new method \n",
    "    # id_train = idDeckTrain\n",
    "    # id_valid = idDeckValid\n",
    "    \n",
    "    idMapYTrain = {} \n",
    "    \n",
    "    # -- Reading Y = id -> [ Sentence(list of char[int] ) ]\n",
    "    with open( TRAINING_LABELS, 'r') as f:\n",
    "        specific = '?'\n",
    "        for lines in f:\n",
    "            yWithId = lines.split('\\n')[0].split(' ', 1)\n",
    "            idMapYTrain[ yWithId[0] ] = []\n",
    "            judgeSpecific = False\n",
    "            for char in yWithId[1] :\n",
    "                if char == '<' :\n",
    "                    judgeSpecific = True;                \n",
    "                if( judgeSpecific ):\n",
    "                    if char == '>':\n",
    "                        idMapYTrain[ yWithId[0] ].append( char_map[specific] )\n",
    "                        judgeSpecific = False\n",
    "                else :\n",
    "                    idMapYTrain[ yWithId[0] ].append( char_map[char.lower()] )\n",
    "    idMapYValid = {} \n",
    "    checkPara = 0\n",
    "    # -- Reading Y = id -> [ Sentence(list of char[int] ) ]\n",
    "    with open( VALIDATION_LABELS, 'r') as f:\n",
    "        specific = '?'\n",
    "        for lines in f:\n",
    "            yWithId = lines.split('\\n')[0].split(' ', 1)\n",
    "            idMapYValid[ yWithId[0] ] = []\n",
    "            judgeSpecific = False\n",
    "            for char in yWithId[1] :\n",
    "                if char == '(' and checkPara == 0 : \n",
    "                    checkPara = 3\n",
    "                elif checkPara > 0 :\n",
    "                    checkPara -= 1\n",
    "                    if checkPara == 0 :\n",
    "                        idMapYValid[ yWithId[0] ].append( char_map[specific] )     \n",
    "                else :\n",
    "                    if char == '<' :\n",
    "                        judgeSpecific = True;                \n",
    "                    if( judgeSpecific ):\n",
    "                        if char == '>':\n",
    "                            idMapYValid[ yWithId[0] ].append( char_map[specific] )\n",
    "                            judgeSpecific = False\n",
    "                    else :\n",
    "                        idMapYValid[ yWithId[0] ].append( char_map[char.lower()] )\n",
    "    \n",
    "    # -- concatenate X and Y\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    \n",
    "    for id in idDeckTrain:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_train.append( floatX( concateId )  )\n",
    "        y_train.append( floatX( idMapYTrain[id]) )\n",
    "        \n",
    "    for id in idDeckValid:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_valid.append( floatX( concateId ) )\n",
    "        y_valid.append( floatX( idMapYValid[id]) )\n",
    "        \n",
    "    print \"Dataset Finish : \"\n",
    "    print \"total x_train = \" + str(len(x_train))\n",
    "    print x_train[0]\n",
    "    print \"total y_train = \" + str(len(y_train))\n",
    "    print y_train[0]\n",
    "    print \"total x_valid = \" + str(len(x_valid))\n",
    "    print x_valid[0]\n",
    "    print \"total y_valid = \" + str(len(y_valid))\n",
    "    print y_valid[0]\n",
    "    \n",
    "    return char_map, char_unMap, x_train, y_train, x_valid, y_valid\n",
    "\n",
    "def floatX(x):\n",
    "    return np.asarray(x, dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "char_map, char_unMap, x_train, y_train, x_valid, y_valid = readDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 363)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 500 -> 3722 / 195\n",
    "# 600 -> 3821 / 201\n",
    "# 3000 -> 3937 / 207\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max( [X.shape[0] for X in x_train] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py\n",
    "    \n",
    "    Convert a list of matrices into batches of uniform length\n",
    "    :parameters:\n",
    "        - X : list of np.ndarray\n",
    "            List of matrices\n",
    "        - length : int\n",
    "            Desired sequence length.  Smaller sequences will be padded with 0s,\n",
    "            longer will be truncated.\n",
    "        - batch_size : int\n",
    "            Mini-batch size\n",
    "    :returns:\n",
    "        - X_batch : np.ndarray\n",
    "            Tensor of time series matrix batches,\n",
    "            shape=(n_batches, length, batch_size, n_features)\n",
    "        - X_mask : np.ndarray\n",
    "            shape=(n_batches, length, batch_size)\n",
    "            Mask denoting whether to include each time step of each time series\n",
    "            matrix\n",
    "    '''\n",
    "    \n",
    "def make_batches_X(X, length, batch_size=30):\n",
    "    n_batches = len(X)//batch_size\n",
    "    X_batch = np.zeros( (n_batches, batch_size, length, X[0].shape[1]),\n",
    "                         dtype=theano.config.floatX)\n",
    "    X_mask  = np.zeros( (n_batches, length, batch_size ), \n",
    "                         dtype=theano.config.floatX)\n",
    "    \n",
    "    for b in range(n_batches): \n",
    "        for n in range(batch_size): # go thorough batch size       \n",
    "            X_m = X[b*batch_size + n] # seq_length X feature dim            \n",
    "            X_batch[b, n, :X_m.shape[0]] = X_m[:length]\n",
    "            X_mask[b, :X_m.shape[0], n] = 1\n",
    "            \n",
    "    return X_batch, X_mask\n",
    "\n",
    "def make_batches_Y( X, length, batch_size=30):\n",
    "    n_batches = len(X)//batch_size\n",
    "    \n",
    "    X_batch = np.zeros( (n_batches, length, batch_size ), dtype='float32')\n",
    "    \n",
    "    X_mask = np.zeros(X_batch.shape, dtype=theano.config.floatX)\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        for n in range(batch_size):\n",
    "            X_m = X[ b*batch_size + n ]\n",
    "            X_batch[b, :X_m.shape[0], n ] = X_m[:length]\n",
    "            X_mask[b, :X_m.shape[0], n] = 1\n",
    "    return X_batch, X_mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "\n",
    "# Find the longest sequence\n",
    "length_x = max( max( [X.shape[0] for X in x_train] ),\n",
    "                max( [X.shape[0] for X in x_valid] ))\n",
    "\n",
    "length_y = max( max( [X.shape[0] for X in y_train] ),\n",
    "                max( [X.shape[0] for X in y_valid] ))\n",
    "\n",
    "# Convert to batches of time series of uniform length\n",
    "# x_train_mask: seq_length X batch_size\n",
    "# y_train_mask: output_length X batch_size\n",
    "# y_pred_mask = x_train_mask, since pred by sequence\n",
    "x_train, x_train_mask = make_batches_X(x_train, length_x, batch_size)\n",
    "y_train, y_train_mask = make_batches_Y(y_train, length_y, batch_size)\n",
    "x_valid, x_valid_mask = make_batches_X(x_valid, length_x, batch_size)\n",
    "y_valid, y_valid_mask = make_batches_Y(y_valid, length_y, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 40, 501, 363)\n",
      "(97, 132, 40)\n",
      "(103, 40, 501, 33)\n",
      "(103, 132, 40)\n",
      "================mask================\n",
      "(97, 501, 40)\n",
      "(97, 132, 40)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_valid.shape\n",
    "print y_valid.shape\n",
    "print \"================mask================\"\n",
    "print x_train_mask.shape\n",
    "print y_train_mask.shape\n",
    "\n",
    "# print x_train_mask[0][400][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit by NLTK package of measure edit distance\n",
    "'''\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (c1 != c2)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def check_label_error( real , predict, transpositions=False):\n",
    "    ## length of real >= length of predict\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(predict)\n",
    "    len2 = len(real)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, predict, real, transpositions=transpositions)\n",
    "            \n",
    "    return lev[len1][len2]*1.0/len2\n",
    "\n",
    "\n",
    "def clean_up( y ):\n",
    "    \"\"\"\n",
    "    for final output clean up\n",
    "    B(a − ab−) = B(−aa − −abb) = aab\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            if y[i-1] != y[i]:\n",
    "                answer += y[i]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def remap_back( y , actual):\n",
    "    answer = \"\"\n",
    "    \n",
    "    for i in y:\n",
    "        answer += char_unMap[i]\n",
    "    \n",
    "    if not actual:\n",
    "        answer = clean_up(answer)\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def decode_all_actual( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = np.swapaxes( y, 0 , 1)[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "\n",
    "def decode_all_pred( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = y[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "def check_all( y, y_mask, y_pred, y_pred_mask, batch_size ):\n",
    "    \n",
    "    actual = decode_all_actual( y, y_mask, batch_size , True )\n",
    "    predict = decode_all_pred( y_pred, y_pred_mask, batch_size , False )\n",
    "    \n",
    "    error = 0.\n",
    "    for a,b in zip (actual, predict):\n",
    "        error += check_label_error(a,b)\n",
    "    \n",
    "    return error/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class end_to_end():  \n",
    "    def __init__(self, input_shape, max_seq_length, hidden_layer,\n",
    "                 batch, max_epochs, output_num_units,\n",
    "                 patience, up_learning_rate, file_name):\n",
    "        \n",
    "        self.input_shape = input_shape # [batch, dim]\n",
    "        \n",
    "        self.hidden_layer = hidden_layer # hidden [l1, l2, l3]\n",
    "        self.output_num_units = output_num_units # [ # of class ]\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.up_learning_rate = up_learning_rate\n",
    "                 \n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_params = None\n",
    "        \n",
    "        self.train_history_ = []\n",
    "        self.epochs = 0\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        \"\"\"\n",
    "        input data type\n",
    "        y_hat : T x B x C+1\n",
    "        y : L x B\n",
    "        y_hat_mask : T x B\n",
    "        y_mask : L x B\n",
    "        \"\"\"\n",
    "        \n",
    "        print \"Why model build so long ... \"\n",
    "        # T x B x F\n",
    "        # B X T X F (Lasagne format)\n",
    "        x = T.tensor3('X', dtype=theano.config.floatX)\n",
    "        # L x B\n",
    "        y = T.matrix ('y', dtype=theano.config.floatX)\n",
    "\n",
    "        # L x B\n",
    "        y_mask = T.matrix('y_mask', dtype=theano.config.floatX)\n",
    "        # T x B\n",
    "        y_hat_mask = T.matrix('y_hat_mask', dtype=theano.config.floatX)\n",
    "    \n",
    "        # Min/max sequence length\n",
    "        MAX_LENGTH = max_seq_length\n",
    "        \n",
    "        # Number of training sequences in each batch\n",
    "        N_BATCH = batch\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # dEEP lSTM\n",
    "        #===========================================================================================\n",
    "        print \"Input : (N_BATCH) : \" + str(N_BATCH) +\", ML :\" + str(MAX_LENGTH) + \"ss :\" + str(self.input_shape[1])\n",
    "        \n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        l_in     = lasagne.layers.InputLayer( shape=( N_BATCH, MAX_LENGTH, self.input_shape[1] ) )\n",
    "        # l_in_gau = lasagne.layers.GaussianNoiseLayer( l_in, sigma=0.5 )\n",
    "        \n",
    "        # LSTM layer 1\n",
    "        l_forward_1   = lasagne.layers.LSTMLayer(l_in, num_units=hidden_layer[0], learn_init=True, peepholes=True)\n",
    "        l_backward_1  = lasagne.layers.LSTMLayer(l_in, num_units=hidden_layer[0], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_1 = ElemwiseSumLayer( [l_forward_1, l_backward_1] )\n",
    "\n",
    "        # LSTM layer 2\n",
    "        l_forward_2   = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], learn_init=True, peepholes=True)\n",
    "        l_backward_2  = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_2 = ElemwiseSumLayer( [l_forward_2, l_backward_2] )\n",
    "        \n",
    "        # LSTM layer 3\n",
    "        l_forward_3   = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], learn_init=True, peepholes=True)\n",
    "        l_backward_3  = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_3 = ElemwiseSumLayer( [l_forward_3, l_backward_3] )\n",
    "        \n",
    "        # LSTM layer 4\n",
    "        l_forward_4   = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], learn_init=True, peepholes=True)\n",
    "        l_backward_4  = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_4 = ElemwiseSumLayer( [l_forward_4, l_backward_4] )\n",
    "        \n",
    "        \n",
    "        l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_4, (N_BATCH*MAX_LENGTH, hidden_layer[3])  )\n",
    "\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # COMMON SETUP\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Our output layer is a simple dense connection\n",
    "        l_recurrent_out      = lasagne.layers.DenseLayer( l_reshape, num_units=output_num_units[0] , nonlinearity=lasagne.nonlinearities.identity)\n",
    "        \n",
    "        # Now, reshape the output back to the RNN format\n",
    "        l_out_shp            = lasagne.layers.ReshapeLayer( l_recurrent_out, (N_BATCH, MAX_LENGTH, output_num_units[0]) )\n",
    "        \n",
    "        # dimshuffle to shape format (input_seq_len, batch_size, num_classes + 1)\n",
    "        l_out_shp_ctc        = lasagne.layers.DimshuffleLayer( l_out_shp, (1, 0, 2))\n",
    "\n",
    "        l_out_softmax        = lasagne.layers.NonlinearityLayer( l_recurrent_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        l_out_softmax_shp    = lasagne.layers.ReshapeLayer( l_out_softmax, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # since we use gaussian noise in input, False means use noise, True means dont use noise\n",
    "        output_lin_ctc_train = lasagne.layers.get_output( l_out_shp_ctc, x, deterministic=False)\n",
    "        output_softmax_train = lasagne.layers.get_output( l_out_softmax_shp, x, deterministic=False)\n",
    "        \n",
    "        output_lin_ctc_val   = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=True)\n",
    "        output_softmax_val   = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=True)\n",
    "        \n",
    "        self.all_params      = lasagne.layers.get_all_params(l_out_shp)\n",
    "\n",
    "        # the CTC cross entropy between y and linear output network\n",
    "        pseudo_cost = ctc_cost_2.pseudo_cost(\n",
    "            y, output_lin_ctc_train, y_mask, y_hat_mask,\n",
    "            skip_softmax=True)\n",
    "        \n",
    "        \n",
    "        pseudo_cost_grad = T.grad(pseudo_cost.mean(), self.all_params)\n",
    "        true_cost        = ctc_cost_2.cost(y, output_softmax_train.dimshuffle(1, 0, 2), y_mask, y_hat_mask)\n",
    "        cost             = T.mean(true_cost)\n",
    "        updates          = lasagne.updates.rmsprop(pseudo_cost_grad, self.all_params, learning_rate = self.up_learning_rate)\n",
    "\n",
    "        self.train = theano.function(\n",
    "            inputs = [x, y, y_hat_mask, y_mask],\n",
    "            outputs = [ pseudo_cost.mean(), cost, output_softmax_train ],\n",
    "#             outputs = [output_lin_ctc, output_softmax, cost],\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        self.predict = theano.function( \n",
    "            inputs=[x], \n",
    "            outputs = [ output_softmax_val] \n",
    "        )\n",
    "\n",
    "        \n",
    "    # x_mask and y_mask is the same\n",
    "    # x_test_mask and y_test_mask is the same\n",
    "    def fit(self, x_train, y_train, x_test,  y_test , x_mask, y_mask, x_test_mask, y_test_mask ):\n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        for i in range(self.max_epochs):\n",
    "            self.epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            cs = 0.\n",
    "            pseudo_cs = 0.\n",
    "            for index in range(len(x_train)):\n",
    "                pseudo, cost, output_softmax = self.train( x_train[index] , y_train[index],\n",
    "                                                          x_mask[index], y_mask[index])\n",
    "                cs += cost\n",
    "                pseudo_cs += pseudo\n",
    "                gg = index\n",
    "                if index % 30 == 0:\n",
    "                    print index, pseudo, cost\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            cs /= len(x_train)\n",
    "            pseudo_cs /= len(x_train)\n",
    "            \n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "            if self.epochs <= 1:\n",
    "                previous_cs = \"-\"\n",
    "            else:\n",
    "                previous_cs = self.train_history_[-1]['cost']\n",
    "            print \"\\n===============================\"\n",
    "            print 'epoch {0} : pseudo= {1}, cost= {2}, previous_cost= {3}, train_time = {4} s'.format(self.epochs,\n",
    "                                                                                                      pseudo_cs, \n",
    "                                                                                                      cs,\n",
    "                                                                                                      previous_cs, \n",
    "                                                                                                      time() - t0)\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            t0 = time()\n",
    "            \n",
    "#             print np.argmax(output_softmax[:],axis=2)\n",
    "#             print decode_all( np.argmax(output_softmax[:],axis=2) , x_train_mask[gg],  batch_size , True)\n",
    "            \n",
    "            # save model first\n",
    "            # dont know the LER performance yet\n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "    \n",
    "\n",
    "            label_error_rate = 0.\n",
    "            print len(x_test)\n",
    "            for index in range( len(x_test) ) :\n",
    "\n",
    "                prepre = self.predict( x_test[index])\n",
    "                \n",
    "                label_error_rate += check_all( y_test[index], y_test_mask[index],\n",
    "                                              np.argmax(prepre[0],axis=2) , x_test_mask[index],\n",
    "                                              batch_size)\n",
    "                \n",
    "                if (( index + self.epochs ) % 4 == 0 ):\n",
    "                    ## print actual\n",
    "                    y_actual = decode_all_actual( y_test[index], y_test_mask[index], batch_size , True)\n",
    "\n",
    "                    ## print pred\n",
    "                    y_predict = decode_all_pred( np.argmax(prepre[0],axis=2) , x_test_mask[index],  batch_size , False)\n",
    "                \n",
    "            label_error_rate /= len(x_test)\n",
    "            self.train_history_.append({\"epoch\":self.epochs, \"cost\": cs, \"LER\":label_error_rate})\n",
    "            \n",
    "            print '\\t\\t\\t val_= {0}, test_time  = {1} s'.format(label_error_rate, time() - t0)\n",
    "            print \"===============================\\n\"\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            for a , b in zip (y_actual,y_predict):\n",
    "                print \"Target== \",a \n",
    "                print \"\\tAns => \",b\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            \"\"\"\n",
    "            should use cost do early stopping\n",
    "            \"\"\"\n",
    "            current_cs = self.train_history_[-1]['cost']\n",
    "            current_epoch = self.train_history_[-1]['epoch']\n",
    "            if current_cs < self.best_valid:\n",
    "                self.best_valid = current_cs\n",
    "                self.best_valid_epoch = current_epoch\n",
    "                self.best_params = [w.get_value() for w in self.all_params]\n",
    "                cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "            elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "                print \"\"\n",
    "                print \"Early stopping.\"\n",
    "                print self.best_valid_epoch,self.best_valid\n",
    "                print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)              \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "            \"\"\"\n",
    "            can not use label error rate do early stopping\n",
    "            \"\"\"\n",
    "#             current_ler = self.train_history_[-1]['LER']\n",
    "#             current_epoch = self.train_history_[-1]['epoch']\n",
    "#             if current_ler < self.best_valid:\n",
    "# #                 print \"********************* Now best ************************\"\n",
    "#                 sys.stdout.flush()\n",
    "#                 self.best_valid = current_ler\n",
    "#                 self.best_valid_epoch = current_epoch\n",
    "#                 self.best_params = [w.get_value() for w in self.all_params]\n",
    "# #                 cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "#             elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "#                 print \"\"\n",
    "#                 print \"Early stopping.\"\n",
    "#                 print self.best_valid_epoch,self.best_valid\n",
    "#                 print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n",
    "#                 sys.stdout.flush()                \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "\n",
    "#     def prediction(self, x, x_mask) :\n",
    "        \n",
    "#         abc =  self.predict(x, x_mask)\n",
    "        \n",
    "#         return np.argmax(abc[0], axis = 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/nntools/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n",
      "/home/pika/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1017: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_valid.shape\n",
    "print y_valid.shape\n",
    "print \"================mask================\"\n",
    "print x_train_mask.shape\n",
    "print y_train_mask.shape\n",
    "\n",
    "# print x_train_mask[0][400][0]\n",
    "(98, 40, 2784, 33)\n",
    "(98, 254, 40)\n",
    "(5, 40, 2784, 33)\n",
    "(5, 254, 40)\n",
    "================mask================\n",
    "(98, 2784, 40)\n",
    "(98, 254, 40)\n",
    "'''\n",
    "\n",
    "model4L250with414 = end_to_end (\n",
    "    input_shape      = (1, x_train.shape[3] ) , # batch of 1, (110, 30, 777, 117)\n",
    "    max_seq_length = x_train.shape[2],\n",
    "    hidden_layer     = [ 250, 250, 250, 250 ], # maximum layer to LSTM 3 layer only\n",
    "    batch            = batch_size, \n",
    "    max_epochs       = 300, \n",
    "    output_num_units = [ len(char_unMap) ],\n",
    "    up_learning_rate = 0.0001, \n",
    "    patience         = 10,\n",
    "    file_name = \"model4L250with414\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time model4L250with414.fit( x_train, y_train, x_valid, y_valid, x_train_mask, y_train_mask , x_valid_mask, y_valid_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "No Guassian error in\n",
    "'''\n",
    "#\n",
    "model4L250with414 = end_to_end (\n",
    "    input_shape      = (1, x_train.shape[3] ) , # batch of 1, (110, 30, 777, 117)\n",
    "#     input_shape = (1, 117),\n",
    "    max_seq_length = x_train.shape[2],\n",
    "# hidden_layer     = [ 200, 200 , 200 ], # maximum layer to LSTM 3 layer only\n",
    "    hidden_layer     = [ 250, 250, 250 , 250 ], # maximum layer to LSTM 3 layer only\n",
    "    batch            = batch_size, \n",
    "    max_epochs       = 100, \n",
    "    eval_size        = 0.1, \n",
    "    output_num_units = [ len(char_unMap) ],\n",
    "    up_learning_rate = 0.0001, \n",
    "    up_momentum      = 0.9, \n",
    "    patience         = 5,\n",
    "    file_name = \"model4L250with414\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time model4L250with414.fit( x_train, y_train, x_valid, y_valid, x_train_mask, y_train_mask , x_valid_mask, y_valid_mask )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Process once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "[ Doing : ]\n",
    "    - Read character from *.txt to build the corpus \n",
    "\n",
    "[ Result : ]\n",
    "\n",
    "-- Without Develope Data:\n",
    "\n",
    "Building corpus without develope data:\n",
    "set([' ', \"'\", '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "Total len is : 31\n",
    "Done Write to [./dataCebuano/corpus.txt]\n",
    "\n",
    "-- With Develope Data:\n",
    "\n",
    "Building corpus with develope data:\n",
    "set([' ', \"'\", ')', '(', '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "Total len is : 33\n",
    "Done Write to [./dataCebuano/corpus.txt]\n",
    "'''\n",
    "\n",
    "TRAINING_LABEL = \"./dataCebuano/train.txt\"\n",
    "DEVELOPE_LABEL = \"./dataCebuano/dev.txt\"\n",
    "WRITING_TO = \"./dataCebuano/corpus.txt\"\n",
    "\n",
    "# Special part to build corpus\n",
    "def buildCharacterCorpus(withDevelopeData = False) :\n",
    "    corpus = set( )\n",
    "    with open(TRAINING_LABEL, 'r') as ft:\n",
    "        for lines in ft:\n",
    "            for word in lines.split('\\n')[0].split(' ')[1:]:\n",
    "                if word[0] == '<':\n",
    "                    # We take every <...> as <unk> \n",
    "                    unKnownTag = '?'\n",
    "                    corpus.add( unKnownTag ) \n",
    "                else:\n",
    "                    for char in word:\n",
    "                        corpus.add(char.lower())\n",
    "    \n",
    "    if( withDevelopeData ) :\n",
    "        with open(DEVELOPE_LABEL, 'r') as fd:\n",
    "            for lines in fd:\n",
    "                for word in lines.split('\\n')[0].split(' ')[1:]:\n",
    "                    if word[0] == '<':\n",
    "                        unKnownTag = '?'\n",
    "                        corpus.add( unKnownTag ) \n",
    "                        # corpus.add(word.split('\\n')[0].lower())\n",
    "                    else:\n",
    "                        for char in word:\n",
    "                            corpus.add(char.lower())\n",
    "    \n",
    "    corpus.add(' ')\n",
    "    print \"Building corpus with\" + (\"\" if withDevelopeData else \"out\") + \" develope data:\"\n",
    "    print corpus\n",
    "    print \"Total len is : \" + str(len(corpus))\n",
    "    \n",
    "    \n",
    "    with open(WRITING_TO, 'w+') as fw:\n",
    "        for item in corpus:\n",
    "            fw.write(item + '\\n')        \n",
    "    print \"Done Write to [\" + WRITING_TO + \"]\"\n",
    "    return corpus\n",
    "\n",
    "trainingCorpus = buildCharacterCorpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Max len is  501\n",
      "Total training data 4157\n",
      "MIssion ComplEte\n"
     ]
    }
   ],
   "source": [
    "# import theano\n",
    "# from theano import tensor as T\n",
    "# from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search as gs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import repeat\n",
    "\n",
    "#proprocessor\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n",
    "\n",
    "# TRAINING_DATA   = './dataCebuano/train.f0_ffv_fbank.fea'\n",
    "# VALIDATION_DATA = './dataCebuano/dev.f0_ffv_fbank.fea'\n",
    "# TRAIN_ARK  = './dataCebuano/train_Normalized_SELFVALID.ark'\n",
    "# TRAIN_ARK  = './dataCebuano/train_Normalized_SELFVALID_LenLessThen600.ark'\n",
    "TRAINING_DATA   = './dataCebuano/dev.f0_ffv_fbank.fea'\n",
    "TRAIN_ARK  = './dataCebuano/dev_Normalized_SELFVALID_LenLessThen500.ark'\n",
    "\n",
    "\n",
    "def normalizeAndFilterData ( maxLength = 3000) :\n",
    "    # Dataset\n",
    "    x_data = []\n",
    "    x_name = []\n",
    "    record_seq = []\n",
    "\n",
    "    with open( TRAINING_DATA, 'r') as f:\n",
    "        for lines in f: \n",
    "            if '[' in lines :\n",
    "                id = lines.split(' ')[0]\n",
    "                x_name.append(id)\n",
    "                seq = 0\n",
    "            elif ']' in lines :\n",
    "                if seq > maxLength :\n",
    "                    x_name.pop()\n",
    "                    del x_data[-seq:]\n",
    "                else :\n",
    "                    seq += 1\n",
    "                    record_seq.append(seq)\n",
    "                    x_data.append([float(x) for x in lines.split(' ') [2:-1] ])\n",
    "            else :\n",
    "                seq += 1\n",
    "                x_data.append([float(x) for x in lines.split(' ') [2:-1] ])\n",
    "\n",
    "    print \"Normalized\"\n",
    "    x_data = preprocessing.scale( np.array(x_data) )\n",
    "\n",
    "    # x_data[separate+1]\n",
    "    # x_name\n",
    "    bitch = record_seq.index(max(record_seq))\n",
    "    print \"Max len is \", record_seq[bitch]\n",
    "    \n",
    "    print \"Total training data\", len(x_name)\n",
    "\n",
    "    # trainF = open(TRAIN_ARK, 'w+')\n",
    "    with open(TRAIN_ARK, 'w+') as trainF:\n",
    "        currentSeq = 0\n",
    "        for nameIndex in range( len(x_name) ):\n",
    "\n",
    "            trainF.write(str(x_name[nameIndex])+\"  [\\n\")\n",
    "\n",
    "            for valueIndex in range( (record_seq[nameIndex]) ):\n",
    "                for featureIndex in range( len(x_data[0]) ):\n",
    "                    if ( featureIndex+1) != len(x_data[0] ) :\n",
    "                        trainF.write( str(x_data[ currentSeq  + valueIndex][featureIndex]) + \" \")\n",
    "                    else:\n",
    "                        trainF.write( str(x_data[ currentSeq  + valueIndex][featureIndex]) )\n",
    "                if valueIndex+1 == (record_seq[nameIndex]):\n",
    "                    trainF.write(\" ]\\n\")\n",
    "                else:\n",
    "                    trainF.write(\"\\n\")\n",
    "            currentSeq += record_seq[nameIndex]\n",
    "\n",
    "    print \"MIssion ComplEte\"\n",
    "\n",
    "\n",
    "normalizeAndFilterData(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
