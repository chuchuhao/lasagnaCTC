{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC version 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Paper Reference\n",
    "- [Connectionist Temporal Classification (IDSIA)](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf)\n",
    "\n",
    "\n",
    "### English Test\n",
    "First we test English\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n",
      "/home/pika/nntools/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "# Include And \n",
    "import sys\n",
    "sys.path.append(\"/home/pika/nntools/\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from theano_toolkit import utils as U\n",
    "from theano_toolkit import updates\n",
    "from theano.printing import Print\n",
    "from theano_toolkit.parameters import Parameters\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "from time import time\n",
    "\n",
    "import ctc_cost_2\n",
    "\n",
    "import cPickle\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus zsize: 3733\n",
      "{' ': 0, \"'\": 1, '*': 32, '-': 2, '.': 31, '?': 3, '_': 4, 'a': 5, 'c': 6, 'b': 7, 'e': 8, 'd': 9, 'g': 10, 'f': 11, 'i': 12, 'h': 13, 'k': 14, 'j': 15, 'm': 16, 'l': 17, 'o': 18, 'n': 19, 'q': 20, 'p': 21, 's': 22, 'r': 23, 'u': 24, 't': 25, 'w': 26, 'v': 27, 'y': 28, 'x': 29, 'z': 30}\n",
      "poor memory\n",
      "Dataset Finish : \n",
      "total x_train = 3937\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "total y_train = 3937\n",
      "[ 3653.   800.  1930.  1479.  1195.  2228.]\n",
      "total x_valid = 207\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "total y_valid = 207\n",
      "[  818.   678.  1505.  3284.   357.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Doing : \n",
    "    - Read data and return x_train, y_train\n",
    "'''\n",
    "TRAINTXT = './dataCebuano/train.txt'\n",
    "# DEVEPTXT = './dataCebuano/dev.txt'\n",
    "DEVEPTXT = './dataCebuano/tlkagk/large_dev.text'\n",
    "\n",
    "CHAR_CORPUS = \"./dataCebuano/corpus.txt\"\n",
    "# WORD_CORPUS = './dataCebuano/courpus.txt'\n",
    "WORD_CORPUS = './dataCebuano/word_courpus.txt'\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n",
    "\n",
    "def readDataSet () :\n",
    "    \n",
    "    # -- Mapping char to integer and reverse\n",
    "    word_map = {}    # eg. { a -> 1 }\n",
    "    word_unMap = {}  # eg. { 1 -> a }\n",
    "    with open( WORD_CORPUS, 'r' ) as f:\n",
    "        word_index = 0\n",
    "        for lines in f:\n",
    "            word = lines.split('\\n')[0]\n",
    "            if len(word) == 0:\n",
    "                print \"Error  in Len \"\n",
    "                return\n",
    "            else:\n",
    "                word_map  [ word       ] = word_index\n",
    "                word_unMap[ word_index ] = word\n",
    "                word_index += 1\n",
    "                \n",
    "    print 'Corpus zsize:', len(word_map)\n",
    "    \n",
    "    # -- Mapping char to integer and reverse\n",
    "    char_map = {}    # eg. { a -> 1 }\n",
    "    char_unMap = {}  # eg. { 1 -> a }\n",
    "    with open( CHAR_CORPUS, 'r' ) as f:\n",
    "        char_index = 0\n",
    "        for lines in f:\n",
    "            char_map  [ lines[:-1] ] = char_index\n",
    "            char_unMap[ char_index ] = lines[:-1]\n",
    "            char_index += 1\n",
    "        # add Blank, take '.' as blank (not space)\n",
    "        char_map['.'] = char_index\n",
    "        char_unMap[char_index] = '.'\n",
    "        char_index += 1\n",
    "        char_map['*'] = char_index\n",
    "        char_unMap[char_index] = '*'\n",
    "    print char_map\n",
    "    \n",
    "    # ---------------------------- Build data\n",
    "    idMapX = {}\n",
    "    idMapY = {}\n",
    "    idDeck = []\n",
    "    with open(TRAINTXT, 'r') as f:\n",
    "        for lines in f:\n",
    "            line = lines.split('\\n')[0]\n",
    "            idline = line.split(' ', 1)\n",
    "            id = idline[0]\n",
    "            idDeck.append(id)\n",
    "            idMapX[id] = []\n",
    "            idMapY[id] = []\n",
    "            # X\n",
    "            judgeSpecific = False\n",
    "            for ch in idline[1]:\n",
    "                ch = ch.lower()\n",
    "                if ch == '<' :\n",
    "                    judgeSpecific = True;                \n",
    "                if( judgeSpecific ):\n",
    "                    if ch == '>':\n",
    "                        ch = '?'\n",
    "                        chBag = vectorized_result( char_map[ch], len(char_map))\n",
    "                        idMapX[id].append([float(x) for x in chBag ])\n",
    "                        # idMapX[id].append( char_map[ch] )\n",
    "                else :\n",
    "                    chBag = vectorized_result( char_map[ch], len(char_map))\n",
    "                    idMapX[id].append([float(x) for x in chBag ])\n",
    "                    #  idMapX[id].append( char_map[ch] )\n",
    "            # Y\n",
    "            for words in idline[1].split(' '):\n",
    "                words=words.lower()\n",
    "                if words[0] == '<':\n",
    "                    words = '?'\n",
    "                # wdBag = vectorized_result( word_map[words], len(word_map))\n",
    "                # idMapY[id].append([float(x) for x in wdBag ])\n",
    "                idMapY[id].append( word_map[words] )\n",
    "                \n",
    "\n",
    "    print \"poor memory\"\n",
    "    \n",
    "    \n",
    "    # Standardlize X (Done in PrepreProcessing )\n",
    "   \n",
    "    # Shuffle the speaker to slice training and validation\n",
    "    totalSpeech = len(idDeck)\n",
    "    slicePointFive = totalSpeech // 20\n",
    "    shuffle(idDeck)\n",
    "    id_train = idDeck[slicePointFive+1:]\n",
    "    id_valid = idDeck[:slicePointFive]\n",
    "    \n",
    "    # -- concatenate X and Y\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    \n",
    "    for id in id_train:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_train.append( floatX( concateId )  )\n",
    "        y_train.append( floatX( idMapY[id]) )\n",
    "        \n",
    "    for id in id_valid:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_valid.append( floatX( concateId ) )\n",
    "        y_valid.append( floatX( idMapY[id]) )\n",
    "        \n",
    "    print \"Dataset Finish : \"\n",
    "    print \"total x_train = \" + str(len(x_train))\n",
    "    print x_train[0]\n",
    "    print \"total y_train = \" + str(len(y_train))\n",
    "    print y_train[0]\n",
    "    print \"total x_valid = \" + str(len(x_valid))\n",
    "    print x_valid[0]\n",
    "    print \"total y_valid = \" + str(len(y_valid))\n",
    "    print y_valid[0]\n",
    "    \n",
    "    return char_map, char_unMap, word_map, word_unMap, x_train, y_train, x_valid, y_valid\n",
    "\n",
    "def floatX(x):\n",
    "    return np.asarray(x, dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "char_map, char_unMap, word_map, word_unMap, x_train, y_train, x_valid, y_valid = readDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 500 -> 3722 / 195\n",
    "# 600 -> 3821 / 201\n",
    "# 3000 -> 3937 / 207\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max( [X.shape[0] for X in x_train] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    https://github.com/craffel/lstm_benchmarks/blob/master/lasagne/experiment.py\n",
    "    \n",
    "    Convert a list of matrices into batches of uniform length\n",
    "    :parameters:\n",
    "        - X : list of np.ndarray\n",
    "            List of matrices\n",
    "        - length : int\n",
    "            Desired sequence length.  Smaller sequences will be padded with 0s,\n",
    "            longer will be truncated.\n",
    "        - batch_size : int\n",
    "            Mini-batch size\n",
    "    :returns:\n",
    "        - X_batch : np.ndarray\n",
    "            Tensor of time series matrix batches,\n",
    "            shape=(n_batches, length, batch_size, n_features)\n",
    "        - X_mask : np.ndarray\n",
    "            shape=(n_batches, length, batch_size)\n",
    "            Mask denoting whether to include each time step of each time series\n",
    "            matrix\n",
    "    '''\n",
    "    \n",
    "def make_batches_X(X, length, batch_size=30):\n",
    "    n_batches = len(X)//batch_size\n",
    "    X_batch = np.zeros( (n_batches, batch_size, length, X[0].shape[1]),\n",
    "                         dtype=theano.config.floatX)\n",
    "    X_mask  = np.zeros( (n_batches, length, batch_size ), \n",
    "                         dtype=theano.config.floatX)\n",
    "    \n",
    "    for b in range(n_batches): \n",
    "        for n in range(batch_size): # go thorough batch size       \n",
    "            X_m = X[b*batch_size + n] # seq_length X feature dim            \n",
    "            X_batch[b, n, :X_m.shape[0]] = X_m[:length]\n",
    "            X_mask[b, :X_m.shape[0], n] = 1\n",
    "            \n",
    "    return X_batch, X_mask\n",
    "\n",
    "def make_batches_Y( X, length, batch_size=30):\n",
    "    n_batches = len(X)//batch_size\n",
    "    \n",
    "    X_batch = np.zeros( (n_batches, length, batch_size ), dtype='float32')\n",
    "    \n",
    "    X_mask = np.zeros(X_batch.shape, dtype=theano.config.floatX)\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        for n in range(batch_size):\n",
    "            X_m = X[ b*batch_size + n ]\n",
    "            X_batch[b, :X_m.shape[0], n ] = X_m[:length]\n",
    "            X_mask[b, :X_m.shape[0], n] = 1\n",
    "    return X_batch, X_mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "\n",
    "# Find the longest sequence\n",
    "length_x = max( max( [X.shape[0] for X in x_train] ),\n",
    "                max( [X.shape[0] for X in x_valid] ))\n",
    "\n",
    "length_y = max( max( [X.shape[0] for X in y_train] ),\n",
    "                max( [X.shape[0] for X in y_valid] ))\n",
    "\n",
    "# Convert to batches of time series of uniform length\n",
    "# x_train_mask: seq_length X batch_size\n",
    "# y_train_mask: output_length X batch_size\n",
    "# y_pred_mask = x_train_mask, since pred by sequence\n",
    "x_train, x_train_mask = make_batches_X(x_train, length_x, batch_size)\n",
    "y_train, y_train_mask = make_batches_Y(y_train, length_y, batch_size)\n",
    "x_valid, x_valid_mask = make_batches_X(x_valid, length_x, batch_size)\n",
    "y_valid, y_valid_mask = make_batches_Y(y_valid, length_y, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 40, 254, 33)\n",
      "(98, 47, 40)\n",
      "(5, 40, 254, 33)\n",
      "(5, 47, 40)\n",
      "================mask================\n",
      "(98, 254, 40)\n",
      "(98, 47, 40)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_valid.shape\n",
    "print y_valid.shape\n",
    "print \"================mask================\"\n",
    "print x_train_mask.shape\n",
    "print y_train_mask.shape\n",
    "\n",
    "# print x_train_mask[0][400][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit by NLTK package of measure edit distance\n",
    "'''\n",
    "# real, hypothesis\n",
    "def wer(r, h):\n",
    "    r = r.split()\n",
    "    h = h.split()\n",
    "    \"\"\"\n",
    "        Calculation of WER with Levenshtein distance.\n",
    "        Works only for iterables up to 254 elements (uint8).\n",
    "        O(nm) time ans space complexity.\n",
    "\n",
    "        >>> wer(\"who is there\".split(), \"is there\".split()) \n",
    "        1\n",
    "        >>> wer(\"who is there\".split(), \"\".split()) \n",
    "        3\n",
    "        >>> wer(\"\".split(), \"who is there\".split()) \n",
    "        3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    d = np.zeros((len(r)+1)*(len(h)+1), dtype=np.uint8)\n",
    "    d = d.reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion    = d[i][j-1] + 1\n",
    "                deletion     = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    return d[len(r)][len(h)] * 1.0 / len(r)\n",
    "\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (c1 != c2)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def check_label_error( real , predict, transpositions=False):\n",
    "    ## length of real >= length of predict\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(predict)\n",
    "    len2 = len(real)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, predict, real, transpositions=transpositions)\n",
    "            \n",
    "    return lev[len1][len2]*1.0/len2\n",
    "\n",
    "\n",
    "def clean_up( y ):\n",
    "    \"\"\"\n",
    "    for final output clean up\n",
    "    B(a − ab−) = B(−aa − −abb) = aab\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            if y[i-1] != y[i]:\n",
    "                answer += y[i]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def remap_back( y , actual):\n",
    "    answer = \"\"\n",
    "    \n",
    "    for i in y:\n",
    "        # answer += char_unMap[i]\n",
    "        answer += word_unMap[i]\n",
    "        answer += ' '\n",
    "    \n",
    "    if not actual:\n",
    "        answer = clean_up(answer)\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def decode_all_actual( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = np.swapaxes( y, 0 , 1)[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "\n",
    "def decode_all_pred( y, y_mask ,batch_size , actual=False):\n",
    "    \"\"\"\n",
    "    y     : label_length X batch_size\n",
    "    y_mask: label_length X batch_size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in xrange(batch_size):\n",
    "        mask = np.swapaxes( y_mask, 0, 1)[i]\n",
    "        ans = y[i]\n",
    "        result.append(remap_back( ans[np.nonzero(mask)] , actual ))\n",
    "    return result\n",
    "\n",
    "def check_all( y, y_mask, y_pred, y_pred_mask, batch_size ):\n",
    "    \n",
    "    actual = decode_all_actual( y, y_mask, batch_size , True )\n",
    "    predict = decode_all_pred( y_pred, y_pred_mask, batch_size , False )\n",
    "    \n",
    "    error = 0.\n",
    "    for a,b in zip (actual, predict):\n",
    "        # error += check_label_error(a,b)\n",
    "        error += wer(a,b)\n",
    "    \n",
    "    return error/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class end_to_end():  \n",
    "    def __init__(self, input_shape, max_seq_length, hidden_layer,\n",
    "                 batch, max_epochs, output_num_units,\n",
    "                 patience, up_learning_rate, file_name):\n",
    "        \n",
    "        self.input_shape = input_shape # [batch, dim]\n",
    "        \n",
    "        self.hidden_layer = hidden_layer # hidden [l1, l2, l3]\n",
    "        self.output_num_units = output_num_units # [ # of class ]\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.up_learning_rate = up_learning_rate\n",
    "                 \n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_params = None\n",
    "        \n",
    "        self.train_history_ = []\n",
    "        self.epochs = 0\n",
    "        \n",
    "        self.file_name = file_name\n",
    "        \n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        \"\"\"\n",
    "        input data type\n",
    "        y_hat : T x B x C+1\n",
    "        y : L x B\n",
    "        y_hat_mask : T x B\n",
    "        y_mask : L x B\n",
    "        \"\"\"\n",
    "        \n",
    "        print \"Why model build so long ... \"\n",
    "        # T x B x F\n",
    "        # B X T X F (Lasagne format)\n",
    "        x = T.tensor3('X', dtype=theano.config.floatX)\n",
    "        # L x B\n",
    "        y = T.matrix ('y', dtype=theano.config.floatX)\n",
    "\n",
    "        # L x B\n",
    "        y_mask = T.matrix('y_mask', dtype=theano.config.floatX)\n",
    "        # T x B\n",
    "        y_hat_mask = T.matrix('y_hat_mask', dtype=theano.config.floatX)\n",
    "    \n",
    "        # Min/max sequence length\n",
    "        MAX_LENGTH = max_seq_length\n",
    "        \n",
    "        # Number of training sequences in each batch\n",
    "        N_BATCH = batch\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # dEEP lSTM\n",
    "        #===========================================================================================\n",
    "        print \"Input : (N_BATCH) : \" + str(N_BATCH) +\", ML :\" + str(MAX_LENGTH) + \"ss :\" + str(self.input_shape[1])\n",
    "        \n",
    "        # Recurrent layers expect input of shape\n",
    "        # (batch size, max sequence length, number of features)\n",
    "        l_in     = lasagne.layers.InputLayer( shape=( N_BATCH, MAX_LENGTH, self.input_shape[1] ) )\n",
    "        l_in_gau = lasagne.layers.GaussianNoiseLayer( l_in, sigma=0.5 )\n",
    "        \n",
    "        # LSTM layer 1\n",
    "        l_forward_1   = lasagne.layers.LSTMLayer(l_in_gau, num_units=hidden_layer[0], learn_init=True, peepholes=True)\n",
    "        l_backward_1  = lasagne.layers.LSTMLayer(l_in_gau, num_units=hidden_layer[0], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_1 = ElemwiseSumLayer( [l_forward_1, l_backward_1] )\n",
    "\n",
    "        '''\n",
    "        # LSTM layer 2\n",
    "        l_forward_2   = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], learn_init=True, peepholes=True)\n",
    "        l_backward_2  = lasagne.layers.LSTMLayer(l_recurrent_1, num_units=hidden_layer[1], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_2 = ElemwiseSumLayer( [l_forward_2, l_backward_2] )\n",
    "        \n",
    "        # LSTM layer 3\n",
    "        l_forward_3   = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], learn_init=True, peepholes=True)\n",
    "        l_backward_3  = lasagne.layers.LSTMLayer(l_recurrent_2, num_units=hidden_layer[2], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_3 = ElemwiseSumLayer( [l_forward_3, l_backward_3] )\n",
    "        \n",
    "        # LSTM layer 4\n",
    "        l_forward_4   = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], learn_init=True, peepholes=True)\n",
    "        l_backward_4  = lasagne.layers.LSTMLayer(l_recurrent_3, num_units=hidden_layer[3], backwards=True, learn_init=True, peepholes=True)\n",
    "        l_recurrent_4 = ElemwiseSumLayer( [l_forward_4, l_backward_4] )\n",
    "        \n",
    "        \n",
    "        l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_4, (N_BATCH*MAX_LENGTH, hidden_layer[3])  )\n",
    "        '''\n",
    "        l_reshape = lasagne.layers.ReshapeLayer(l_recurrent_1, (N_BATCH*MAX_LENGTH, hidden_layer[0])  )\n",
    "\n",
    "        \n",
    "        #===========================================================================================\n",
    "        # COMMON SETUP\n",
    "        #===========================================================================================\n",
    "        \n",
    "        # Our output layer is a simple dense connection\n",
    "        l_recurrent_out      = lasagne.layers.DenseLayer( l_reshape, num_units=output_num_units[0] , nonlinearity=lasagne.nonlinearities.identity)\n",
    "        \n",
    "        # Now, reshape the output back to the RNN format\n",
    "        l_out_shp            = lasagne.layers.ReshapeLayer( l_recurrent_out, (N_BATCH, MAX_LENGTH, output_num_units[0]) )\n",
    "        \n",
    "        # dimshuffle to shape format (input_seq_len, batch_size, num_classes + 1)\n",
    "        l_out_shp_ctc        = lasagne.layers.DimshuffleLayer( l_out_shp, (1, 0, 2))\n",
    "\n",
    "        l_out_softmax        = lasagne.layers.NonlinearityLayer( l_recurrent_out, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        l_out_softmax_shp    = lasagne.layers.ReshapeLayer( l_out_softmax, (N_BATCH, MAX_LENGTH, output_num_units[0] ))\n",
    "        \n",
    "        # since we use gaussian noise in input, False means use noise, True means dont use noise\n",
    "        output_lin_ctc_train = lasagne.layers.get_output( l_out_shp_ctc, x, deterministic=False)\n",
    "        output_softmax_train = lasagne.layers.get_output( l_out_softmax_shp, x, deterministic=False)\n",
    "        \n",
    "        output_lin_ctc_val   = lasagne.layers.get_output(l_out_shp_ctc, x, deterministic=True)\n",
    "        output_softmax_val   = lasagne.layers.get_output(l_out_softmax_shp, x, deterministic=True)\n",
    "        \n",
    "        self.all_params      = lasagne.layers.get_all_params(l_out_shp)\n",
    "    \n",
    "        # the CTC cross entropy between y and linear output network\n",
    "        pseudo_cost = ctc_cost_2.pseudo_cost(\n",
    "            y, output_lin_ctc_train, y_mask, y_hat_mask,\n",
    "            skip_softmax=True)\n",
    "        \n",
    "        \n",
    "        pseudo_cost_grad = T.grad(pseudo_cost.mean(), self.all_params)\n",
    "        true_cost        = ctc_cost_2.cost(y, output_softmax_train.dimshuffle(1, 0, 2), y_mask, y_hat_mask)\n",
    "        cost             = T.mean(true_cost)\n",
    "        \n",
    "        pseudo_cost_scaled_grads = lasagne.updates.total_norm_constraint(pseudo_cost_grad, 5)\n",
    "        updates_rms          = lasagne.updates.rmsprop(pseudo_cost_scaled_grads, self.all_params, learning_rate = self.up_learning_rate)\n",
    "        updates = lasagne.updates.apply_momentum(updates_rms,  self.all_params, momentum=0.9)\n",
    "        \n",
    "        self.train = theano.function(\n",
    "            inputs = [x, y, y_hat_mask, y_mask],\n",
    "            outputs = [ pseudo_cost.mean(), cost, output_softmax_train ],\n",
    "#             outputs = [output_lin_ctc, output_softmax, cost],\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        self.predict = theano.function( \n",
    "            inputs=[x], \n",
    "            outputs = [ output_softmax_val] \n",
    "        )\n",
    "\n",
    "        \n",
    "    # x_mask and y_mask is the same\n",
    "    # x_test_mask and y_test_mask is the same\n",
    "    def fit(self, x_train, y_train, x_test,  y_test , x_mask, y_mask, x_test_mask, y_test_mask ):\n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        for i in range(self.max_epochs):\n",
    "            self.epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            cs = 0.\n",
    "            pseudo_cs = 0.\n",
    "            for index in range(len(x_train)):\n",
    "                pseudo, cost, output_softmax = self.train( x_train[index] , y_train[index],\n",
    "                                                          x_mask[index], y_mask[index])\n",
    "                cs += cost\n",
    "                pseudo_cs += pseudo\n",
    "                gg = index\n",
    "                if index % 10 == 0:\n",
    "                    print index, pseudo, cost\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            cs /= len(x_train)\n",
    "            pseudo_cs /= len(x_train)\n",
    "            \n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "            if self.epochs <= 1:\n",
    "                previous_cs = \"-\"\n",
    "            else:\n",
    "                previous_cs = self.train_history_[-1]['cost']\n",
    "            print \"\\n===============================\"\n",
    "            print 'epoch {0} : pseudo= {1}, cost= {2}, previous_cost= {3}, train_time = {4} s'.format(self.epochs,\n",
    "                                                                                                      pseudo_cs, \n",
    "                                                                                                      cs,\n",
    "                                                                                                      previous_cs, \n",
    "                                                                                                      time() - t0)\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            t0 = time()\n",
    "            \n",
    "#             print np.argmax(output_softmax[:],axis=2)\n",
    "#             print decode_all( np.argmax(output_softmax[:],axis=2) , x_train_mask[gg],  batch_size , True)\n",
    "            \n",
    "            # save model first\n",
    "            # dont know the LER performance yet\n",
    "#             cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "    \n",
    "            y_actual = []\n",
    "            y_predict= []\n",
    "            label_error_rate = 0.\n",
    "            print len(x_test)\n",
    "            for index in range( len(x_test) ) :\n",
    "\n",
    "                prepre = self.predict( x_test[index])\n",
    "                \n",
    "                label_error_rate += check_all( y_test[index], y_test_mask[index],\n",
    "                                              np.argmax(prepre[0],axis=2) , x_test_mask[index],\n",
    "                                              batch_size)\n",
    "                \n",
    "                if (( index + self.epochs ) % 4 == 0 ):\n",
    "                    ## print actual\n",
    "                    y_actual = decode_all_actual( y_test[index], y_test_mask[index], batch_size , True)\n",
    "\n",
    "                    ## print pred\n",
    "                    y_predict = decode_all_pred( np.argmax(prepre[0],axis=2) , x_test_mask[index],  batch_size , False)\n",
    "                \n",
    "            label_error_rate /= len(x_test)\n",
    "            self.train_history_.append({\"epoch\":self.epochs, \"cost\": cs, \"LER\":label_error_rate})\n",
    "            \n",
    "            print '\\t\\t\\t val_= {0}, test_time  = {1} s'.format(label_error_rate, time() - t0)\n",
    "            print \"===============================\\n\"\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            for a , b in zip (y_actual,y_predict):\n",
    "                print \"Target== \",a \n",
    "                print \"\\tAns => \",b\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            \"\"\"\n",
    "            should use cost do early stopping\n",
    "            \"\"\"\n",
    "            current_cs = self.train_history_[-1]['cost']\n",
    "            current_epoch = self.train_history_[-1]['epoch']\n",
    "            if current_cs < self.best_valid:\n",
    "                self.best_valid = current_cs\n",
    "                self.best_valid_epoch = current_epoch\n",
    "                self.best_params = [w.get_value() for w in self.all_params]\n",
    "                cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "            elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "                print \"\"\n",
    "                print \"Early stopping.\"\n",
    "                print self.best_valid_epoch,self.best_valid\n",
    "                print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)              \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "            \"\"\"\n",
    "            can not use label error rate do early stopping\n",
    "            \"\"\"\n",
    "#             current_ler = self.train_history_[-1]['LER']\n",
    "#             current_epoch = self.train_history_[-1]['epoch']\n",
    "#             if current_ler < self.best_valid:\n",
    "# #                 print \"********************* Now best ************************\"\n",
    "#                 sys.stdout.flush()\n",
    "#                 self.best_valid = current_ler\n",
    "#                 self.best_valid_epoch = current_epoch\n",
    "#                 self.best_params = [w.get_value() for w in self.all_params]\n",
    "# #                 cPickle.dump( self , open(\"./\"+self.file_name+\".pkl\",\"wb\"))\n",
    "                \n",
    "#             elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "#                 print \"\"\n",
    "#                 print \"Early stopping.\"\n",
    "#                 print self.best_valid_epoch,self.best_valid\n",
    "#                 print \"Best valid ler {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n",
    "#                 sys.stdout.flush()                \n",
    "#                 for qq in range (len(self.all_params)):\n",
    "#                     self.all_params[qq].set_value( self.best_params[qq] )\n",
    "#                 break\n",
    "\n",
    "\n",
    "#     def prediction(self, x, x_mask) :\n",
    "        \n",
    "#         abc =  self.predict(x, x_mask)\n",
    "        \n",
    "#         return np.argmax(abc[0], axis = 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ONE lAYER\n",
    "'''\n",
    "\n",
    "model4L_Noise_momemtumClip_bottleneckfea = end_to_end (\n",
    "    input_shape      = (1, x_train.shape[3] ) , # batch of 1, (110, 30, 777, 117)\n",
    "    max_seq_length = x_train.shape[2],\n",
    "    hidden_layer     = [ 650, 200, 200, 200 ], # maximum layer to LSTM 3 layer only\n",
    "    batch            = batch_size, \n",
    "    max_epochs       = 300, \n",
    "    output_num_units = [ len(word_unMap) ],\n",
    "    up_learning_rate = 0.00001, \n",
    "    patience         = 7,\n",
    "    file_name = \"LM_model1L(n50)_Noise_momemtumm\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time model4L_Noise_momemtumClip_bottleneckfea.fit( x_train, y_train, x_valid, y_valid, x_train_mask, y_train_mask , x_valid_mask, y_valid_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "＃ Not used below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Process once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "[ Doing : ]\n",
    "    - Read character from *.txt to build the corpus \n",
    "\n",
    "[ Result : ]\n",
    "\n",
    "-- Without Develope Data:\n",
    "\n",
    "Building corpus without develope data:\n",
    "set([' ', \"'\", '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "Total len is : 31\n",
    "Done Write to [./dataCebuano/corpus.txt]\n",
    "\n",
    "-- With Develope Data:\n",
    "\n",
    "Building corpus with develope data:\n",
    "set([' ', \"'\", ')', '(', '-', '?', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "Total len is : 33\n",
    "Done Write to [./dataCebuano/corpus.txt]\n",
    "'''\n",
    "\n",
    "TRAINING_LABEL = \"./dataCebuano/train.txt\"\n",
    "DEVELOPE_LABEL = \"./dataCebuano/dev.txt\"\n",
    "WRITING_TO = \"./dataCebuano/corpus.txt\"\n",
    "\n",
    "# Special part to build corpus\n",
    "def buildCharacterCorpus(withDevelopeData = False) :\n",
    "    corpus = set( )\n",
    "    with open(TRAINING_LABEL, 'r') as ft:\n",
    "        for lines in ft:\n",
    "            for word in lines.split('\\n')[0].split(' ')[1:]:\n",
    "                if word[0] == '<':\n",
    "                    # We take every <...> as <unk> \n",
    "                    unKnownTag = '?'\n",
    "                    corpus.add( unKnownTag ) \n",
    "                else:\n",
    "                    for char in word:\n",
    "                        corpus.add(char.lower())\n",
    "    \n",
    "    if( withDevelopeData ) :\n",
    "        with open(DEVELOPE_LABEL, 'r') as fd:\n",
    "            for lines in fd:\n",
    "                for word in lines.split('\\n')[0].split(' ')[1:]:\n",
    "                    if word[0] == '<':\n",
    "                        unKnownTag = '?'\n",
    "                        corpus.add( unKnownTag ) \n",
    "                        # corpus.add(word.split('\\n')[0].lower())\n",
    "                    else:\n",
    "                        for char in word:\n",
    "                            corpus.add(char.lower())\n",
    "    \n",
    "    corpus.add(' ')\n",
    "    print \"Building corpus with\" + (\"\" if withDevelopeData else \"out\") + \" develope data:\"\n",
    "    print corpus\n",
    "    print \"Total len is : \" + str(len(corpus))\n",
    "    \n",
    "    \n",
    "    with open(WRITING_TO, 'w+') as fw:\n",
    "        for item in corpus:\n",
    "            fw.write(item + '\\n')        \n",
    "    print \"Done Write to [\" + WRITING_TO + \"]\"\n",
    "    return corpus\n",
    "\n",
    "trainingCorpus = buildCharacterCorpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import theano\n",
    "# from theano import tensor as T\n",
    "# from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search as gs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import repeat\n",
    "\n",
    "#proprocessor\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n",
    "\n",
    "TRAINING_DATA   = './dataCebuano/tlkagk/train.dnn.fea'\n",
    "# VALIDATION_DATA = './dataCebuano/dev.f0_ffv_fbank.fea'\n",
    "TRAIN_ARK  = './dataCebuano/tlkagk/train.dnn.fea.lessThen500.ark'\n",
    "# TRAIN_ARK  = './dataCebuano/train_Normalized_SELFVALID_LenLessThen600.ark'\n",
    "\n",
    "\n",
    "def normalizeAndFilterData ( maxLength = 3000) :\n",
    "    # Dataset\n",
    "    x_data = []\n",
    "    x_name = []\n",
    "    record_seq = []\n",
    "\n",
    "    with open( TRAINING_DATA, 'r') as f:\n",
    "        for lines in f: \n",
    "            if '[' in lines :\n",
    "                id = lines.split(' ')[0]\n",
    "                x_name.append(id)\n",
    "                seq = 0\n",
    "            elif ']' in lines :\n",
    "                if seq > maxLength :\n",
    "                    x_name.pop()\n",
    "                    del x_data[-seq:]\n",
    "                else :\n",
    "                    seq += 1\n",
    "                    record_seq.append(seq)\n",
    "                    x_data.append([float(x) for x in lines.split(' ') [2:-1] ])\n",
    "            else :\n",
    "                seq += 1\n",
    "                x_data.append([float(x) for x in lines.split(' ') [2:-1] ])\n",
    "\n",
    "    print \"Normalized\"\n",
    "    x_data = preprocessing.scale( np.array(x_data) )\n",
    "\n",
    "    # x_data[separate+1]\n",
    "    # x_name\n",
    "    bitch = record_seq.index(max(record_seq))\n",
    "    print \"Max len is \", record_seq[bitch]\n",
    "    \n",
    "    print \"Each Length is \", len(x_data[bitch])\n",
    "    print \"Original Total training data\", len(x_name)\n",
    "\n",
    "    # trainF = open(TRAIN_ARK, 'w+')\n",
    "    with open(TRAIN_ARK, 'w+') as trainF:\n",
    "        currentSeq = 0\n",
    "        for nameIndex in range( len(x_name) ):\n",
    "\n",
    "            trainF.write(str(x_name[nameIndex])+\"  [\\n\")\n",
    "\n",
    "            for valueIndex in range( (record_seq[nameIndex]) ):\n",
    "                for featureIndex in range( len(x_data[0]) ):\n",
    "                    if ( featureIndex+1) != len(x_data[0] ) :\n",
    "                        trainF.write( str(x_data[ currentSeq  + valueIndex][featureIndex]) + \" \")\n",
    "                    else:\n",
    "                        trainF.write( str(x_data[ currentSeq  + valueIndex][featureIndex]) )\n",
    "                if valueIndex+1 == (record_seq[nameIndex]):\n",
    "                    trainF.write(\" ]\\n\")\n",
    "                else:\n",
    "                    trainF.write(\"\\n\")\n",
    "            currentSeq += record_seq[nameIndex]\n",
    "\n",
    "    print \"MIssion ComplEte\"\n",
    "\n",
    "\n",
    "normalizeAndFilterData(500)\n",
    "# ORIGINAL 3918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's build dict\n",
      "words is train =  3733\n",
      "words in dev =  3733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sets import Set\n",
    "\n",
    "TRAINTXT = './dataCebuano/train.txt'\n",
    "DEVEPTXT = './dataCebuano/tlkagk/large_dev.text'\n",
    "# DEVEPTXT = './dataCebuano/dev.txt'\n",
    "CORPUS_FILE = './dataCebuano/word_courpus.txt'\n",
    "\n",
    "\n",
    "def buildWordDict():\n",
    "    wordsInTrainandDev =  Set([])\n",
    "    print \"let's build dict\"\n",
    "    with open(TRAINTXT, 'r') as f:\n",
    "        for lines in f:\n",
    "            line = lines.split('\\n')[0]\n",
    "            for words in line.split(' ')[1:]:\n",
    "                words = words.lower()\n",
    "                if words[0] == '<':\n",
    "                    words = '?'\n",
    "                wordsInTrainandDev.add(words)\n",
    "            # print wordsInTrain\n",
    "            # wordsInTrain.add(\n",
    "            # break\n",
    "    print \"words is train = \", len(wordsInTrainandDev)\n",
    "    \n",
    "    '''    \n",
    "    with open(DEVEPTXT, 'r') as f:\n",
    "        for lines in f:\n",
    "            line = lines.split('\\n')[0]\n",
    "            for words in line.split(' ')[1:]:\n",
    "                words = words.lower()\n",
    "                if words[0] == '<':\n",
    "                    words = '?'\n",
    "                wordsInTrainandDev.add(words)\n",
    "            # print wordsInTrain\n",
    "            # wordsInTrain.add(\n",
    "            # break\n",
    "    '''            \n",
    "    with open(CORPUS_FILE, 'w+') as f:\n",
    "        for item in wordsInTrainandDev:\n",
    "            f.write(item + '\\n')\n",
    "\n",
    "    print \"words in dev = \", len(wordsInTrainandDev)\n",
    "\n",
    "buildWordDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus zsize: 3733\n",
      "{' ': 0, \"'\": 1, '*': 32, '-': 2, '.': 31, '?': 3, '_': 4, 'a': 5, 'c': 6, 'b': 7, 'e': 8, 'd': 9, 'g': 10, 'f': 11, 'i': 12, 'h': 13, 'k': 14, 'j': 15, 'm': 16, 'l': 17, 'o': 18, 'n': 19, 'q': 20, 'p': 21, 's': 22, 'r': 23, 'u': 24, 't': 25, 'w': 26, 'v': 27, 'y': 28, 'x': 29, 'z': 30}\n",
      "poor memory\n",
      "Dataset Finish : \n",
      "total x_train = 3937\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "total y_train = 3937\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "total x_valid = 207\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "total y_valid = 207\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Doing : \n",
    "    - Read data and return x_train, y_train\n",
    "'''\n",
    "import numpy as np\n",
    "TRAINTXT = './dataCebuano/train.txt'\n",
    "# DEVEPTXT = './dataCebuano/dev.txt'\n",
    "DEVEPTXT = './dataCebuano/tlkagk/large_dev.text'\n",
    "\n",
    "CHAR_CORPUS = \"./dataCebuano/corpus.txt\"\n",
    "# WORD_CORPUS = './dataCebuano/courpus.txt'\n",
    "WORD_CORPUS = './dataCebuano/word_courpus.txt'\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def vectorized_result ( j , siz) :\n",
    "    e = np.zeros((siz, 1))\n",
    "    e[j] = 1.0\n",
    "    return np.reshape( e, siz)\n",
    "\n",
    "def readDataSet () :\n",
    "    \n",
    "    # -- Mapping char to integer and reverse\n",
    "    word_map = {}    # eg. { a -> 1 }\n",
    "    word_unMap = {}  # eg. { 1 -> a }\n",
    "    with open( WORD_CORPUS, 'r' ) as f:\n",
    "        word_index = 0\n",
    "        for lines in f:\n",
    "            word = lines.split('\\n')[0]\n",
    "            if len(word) == 0:\n",
    "                print \"Error  in Len \"\n",
    "                return\n",
    "            else:\n",
    "                word_map  [ word       ] = word_index\n",
    "                word_unMap[ word_index ] = word\n",
    "                word_index += 1\n",
    "                \n",
    "    print 'Corpus zsize:', len(word_map)\n",
    "    \n",
    "    # -- Mapping char to integer and reverse\n",
    "    char_map = {}    # eg. { a -> 1 }\n",
    "    char_unMap = {}  # eg. { 1 -> a }\n",
    "    with open( CHAR_CORPUS, 'r' ) as f:\n",
    "        char_index = 0\n",
    "        for lines in f:\n",
    "            char_map  [ lines[:-1] ] = char_index\n",
    "            char_unMap[ char_index ] = lines[:-1]\n",
    "            char_index += 1\n",
    "        # add Blank, take '.' as blank (not space)\n",
    "        char_map['.'] = char_index\n",
    "        char_unMap[char_index] = '.'\n",
    "        char_index += 1\n",
    "        char_map['*'] = char_index\n",
    "        char_unMap[char_index] = '*'\n",
    "    print char_map\n",
    "    \n",
    "    # ---------------------------- Build data\n",
    "    idMapX = {}\n",
    "    idMapY = {}\n",
    "    idDeck = []\n",
    "    with open(TRAINTXT, 'r') as f:\n",
    "        for lines in f:\n",
    "            line = lines.split('\\n')[0]\n",
    "            idline = line.split(' ', 1)\n",
    "            id = idline[0]\n",
    "            idDeck.append(id)\n",
    "            idMapX[id] = []\n",
    "            idMapY[id] = []\n",
    "            # X\n",
    "            judgeSpecific = False\n",
    "            for ch in idline[1]:\n",
    "                ch = ch.lower()\n",
    "                if ch == '<' :\n",
    "                    judgeSpecific = True;                \n",
    "                if( judgeSpecific ):\n",
    "                    if ch == '>':\n",
    "                        ch = '?'\n",
    "                        chBag = vectorized_result( char_map[ch], len(char_map))\n",
    "                        idMapX[id].append([float(x) for x in chBag ])\n",
    "                else :\n",
    "                    chBag = vectorized_result( char_map[ch], len(char_map))\n",
    "                    idMapX[id].append([float(x) for x in chBag ])\n",
    "            # Y\n",
    "            for words in idline[1].split(' '):\n",
    "                words=words.lower()\n",
    "                if words[0] == '<':\n",
    "                    words = '?'\n",
    "                wdBag = vectorized_result( word_map[words], len(word_map))\n",
    "                idMapY[id].append([float(x) for x in wdBag ])\n",
    "    '''\n",
    "    with open(DEVEPTXT, 'r') as f:\n",
    "        for lines in f:\n",
    "            line = lines.split('\\n')[0]\n",
    "            idline = line.split(' ', 1)\n",
    "            id = idline[0]\n",
    "            idDeck.append(id)\n",
    "            idMapX[id] = []\n",
    "            idMapY[id] = []\n",
    "            # X\n",
    "            judgeSpecific = False\n",
    "            for ch in idline[1]:\n",
    "                ch = ch.lower()\n",
    "                if ch == '<' :\n",
    "                    judgeSpecific = True;                \n",
    "                if( judgeSpecific ):\n",
    "                    if ch == '>':\n",
    "                        ch = '?'\n",
    "                        chBag = vectorized_result( char_map[ch], len(char_map))\n",
    "                        idMapX[id].append([float(x) for x in chBag ])\n",
    "                else :\n",
    "                    chBag = vectorized_result( char_map[ch], len(char_map))\n",
    "                    idMapX[id].append([float(x) for x in chBag ])\n",
    "            # Y\n",
    "            for words in idline[1].split(' '):\n",
    "                words = words.lower()\n",
    "                if words[0] == '<':\n",
    "                    words = '?'\n",
    "                wdBag = vectorized_result( word_map[words], len(word_map))\n",
    "                idMapY[id].append([float(x) for x in wdBag ])\n",
    "    '''\n",
    "    print \"poor memory\"\n",
    "    \n",
    "    \n",
    "    # Standardlize X (Done in PrepreProcessing )\n",
    "   \n",
    "    # Shuffle the speaker to slice training and validation\n",
    "    totalSpeech = len(idDeck)\n",
    "    slicePointFive = totalSpeech // 20\n",
    "    shuffle(idDeck)\n",
    "    id_train = idDeck[slicePointFive+1:]\n",
    "    id_valid = idDeck[:slicePointFive]\n",
    "    \n",
    "    # -- concatenate X and Y\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    \n",
    "    for id in id_train:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_train.append( floatX( concateId )  )\n",
    "        y_train.append( floatX( idMapY[id]) )\n",
    "        \n",
    "    for id in id_valid:\n",
    "        concateId = []\n",
    "        for i in range( len( idMapX[id] ) ) :\n",
    "            concateId.append( idMapX[id][i] )\n",
    "        x_valid.append( floatX( concateId ) )\n",
    "        y_valid.append( floatX( idMapY[id]) )\n",
    "        \n",
    "    print \"Dataset Finish : \"\n",
    "    print \"total x_train = \" + str(len(x_train))\n",
    "    print x_train[0]\n",
    "    print \"total y_train = \" + str(len(y_train))\n",
    "    print y_train[0]\n",
    "    print \"total x_valid = \" + str(len(x_valid))\n",
    "    print x_valid[0]\n",
    "    print \"total y_valid = \" + str(len(y_valid))\n",
    "    print y_valid[0]\n",
    "    \n",
    "    return char_map, char_unMap, x_train, y_train, x_valid, y_valid\n",
    "\n",
    "def floatX(x):\n",
    "    return np.asarray(x, dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "char_map, char_unMap, x_train, y_train, x_valid, y_valid = readDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
